{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726},{"sourceId":7456613,"sourceType":"datasetVersion","datasetId":4340359},{"sourceId":9992305,"sourceType":"datasetVersion","datasetId":6149797}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Required Libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport torch \nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport random\nimport torchmetrics\nfrom tqdm.notebook import tqdm_notebook as tqdm","metadata":{"_uuid":"84e3262e-3443-45b9-80c7-62a855e7dadd","_cell_guid":"970076ad-faae-4586-9f02-e7b96ac38afe","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T20:30:46.574835Z","iopub.execute_input":"2024-12-04T20:30:46.575709Z","iopub.status.idle":"2024-12-04T20:30:49.032290Z","shell.execute_reply.started":"2024-12-04T20:30:46.575674Z","shell.execute_reply":"2024-12-04T20:30:49.031353Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# **Setting Seed**","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\n\nrandom.seed(42)\n\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\ntorch.cuda.manual_seed_all(42)  \ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nos.environ['PYTHONHASHSEED'] = str(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T20:30:49.033849Z","iopub.execute_input":"2024-12-04T20:30:49.034409Z","iopub.status.idle":"2024-12-04T20:30:49.046849Z","shell.execute_reply.started":"2024-12-04T20:30:49.034380Z","shell.execute_reply":"2024-12-04T20:30:49.046117Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# **Filters**","metadata":{}},{"cell_type":"code","source":"# Define the original filter\nfilters = [\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, -1, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, -1, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 1, -1, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, -1, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, -1, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, -1, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, -1, 1, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, -1, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, -1, 0, 0],\n    [0, 0, 3, 0, 0],\n    [0, 0, -3, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [-1, 0, 0, 0, 0],\n    [0, 3, 0, 0, 0],\n    [0, 0, -3, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [-1, 3, -3, 1, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, -3, 0, 0],\n    [0, 3, 0, 0, 0],\n    [-1, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, -3, 0, 0],\n    [0, 0, 3, 0, 0],\n    [0, 0, -1, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, -3, 0, 0],\n    [0, 0, 0, 3, 0],\n    [0, 0, 0, 0, -1]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 1, -3, 3, -1],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, -1],\n    [0, 0, 0, 3, 0],\n    [0, 0, -3, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, -2, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, -2, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 1, -2, 1, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, -2, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, -1, 2, -1, 0],\n    [0, 2, -4, 2, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, -1, 2, 0, 0],\n    [0, 2, -4, 0, 0],\n    [0, -1, 2, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0 ,0, 0, 0],\n    [0, 2, -4, 2, 0],\n    [0, -1, 2, -1, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 2, -1, 0],\n    [0, 0, -4, 2, 0],\n    [0, 0, 2, -1, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [-1, 2, -2, 2, -1],\n    [2, -6, 8, -6, 2],\n    [-2, 8, -12, 8, -2],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [-1, 2, -2, 0, 0],\n    [2, -6, 8, 0, 0],\n    [-2, 8, -12, 0, 0],\n    [2, -6, 8, 0, 0],\n    [-1, 2, -2, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [-2, 8, -12, 8, -2],\n    [2, -6, 8, -6, 2],\n    [-1, 2, -2, 2, -1]]),\n    np.array([\n    [0, 0, -2, 2, -1],\n    [0, 0, 8, -6, 2],\n    [0, 0, -12, 8, -2],\n    [0, 0, 8, -6, 2],\n    [0, 0, -2, 2, -1]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, -1, 2, -1, 0],\n    [0, 2, -4, 2, 0],\n    [0, -1, 2, -1, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [-1, 2, -2, 2, -1],\n    [2, -6, 8, -6, 2],\n    [-2, 8, -12, 8, -2],\n    [2, -6, 8, -6, 2],\n    [-1, 2, -2, 2, -1]])\n]","metadata":{"_uuid":"7c9f30f1-f7b8-4d3b-b5dd-0601c737ee2a","_cell_guid":"f3a48a48-3279-4f76-a6ef-a25b1fd45ac3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T20:30:49.047894Z","iopub.execute_input":"2024-12-04T20:30:49.048193Z","iopub.status.idle":"2024-12-04T20:30:49.072577Z","shell.execute_reply.started":"2024-12-04T20:30:49.048168Z","shell.execute_reply":"2024-12-04T20:30:49.071664Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"_uuid":"661b8d71-c59c-49ea-a6fc-0dbc65fdcf76","_cell_guid":"93511917-7507-4a0d-9e72-c683cc857c89","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T20:30:49.074088Z","iopub.execute_input":"2024-12-04T20:30:49.074337Z","iopub.status.idle":"2024-12-04T20:30:49.088220Z","shell.execute_reply.started":"2024-12-04T20:30:49.074312Z","shell.execute_reply":"2024-12-04T20:30:49.087400Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"# **Image Preprocessing**","metadata":{}},{"cell_type":"code","source":"def normal_patches(image, patch_size):\n    # Extract height and width of the image\n    height, width = image.shape[:2]\n    \n    # Create an empty list to store patches\n    patches = [\n        # Extract patches of size (patch_size x patch_size) from the image\n        image[i:i + patch_size, j:j + patch_size]\n        for i in range(0, height, patch_size)  # Iterate over rows with step size = patch_size\n        for j in range(0, width, patch_size)   # Iterate over columns with step size = patch_size\n    ]\n    return patches  # Return the list of extracted patches","metadata":{"_uuid":"84b5269e-72fa-4509-a654-0fb55f6ec221","_cell_guid":"590a347f-eb84-447f-bb54-b5f9c2d9c8ae","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T20:30:49.114656Z","iopub.execute_input":"2024-12-04T20:30:49.114895Z","iopub.status.idle":"2024-12-04T20:30:49.119243Z","shell.execute_reply.started":"2024-12-04T20:30:49.114872Z","shell.execute_reply":"2024-12-04T20:30:49.118469Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def reconstruct(patches):\n    # Flatten each patch along its spatial dimensions and combine into a single array\n    row_patches = np.concatenate(\n        [patch.reshape(-1, 3) for patch in patches], axis=0\n    )\n    \n    # Reshape the concatenated patches back into the original image format\n    combined_image = row_patches.reshape(32, 16, 3)  # Assuming image dimensions are known (32x16x3)\n    return combined_image  # Return the reconstructed image","metadata":{"_uuid":"d4d70580-19b6-46a6-85c7-13844657d7f5","_cell_guid":"ebe22106-67ec-4959-bda2-64f7ef6eaf51","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T20:30:49.526347Z","iopub.execute_input":"2024-12-04T20:30:49.526644Z","iopub.status.idle":"2024-12-04T20:30:49.530937Z","shell.execute_reply.started":"2024-12-04T20:30:49.526618Z","shell.execute_reply":"2024-12-04T20:30:49.529932Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def texture_diversity(image):\n    M = image.shape[0]\n    ldiv = 0  # Initialize the diversity score\n\n    # Loop through each color channel\n    for channel in range(3):\n        patch = image[:, :, channel]  # Extract a single channel (R, G, or B)\n        \n        # Calculate the absolute differences between neighboring pixels along different axes\n        ldiv += np.abs(patch[:, :-1] - patch[:, 1:]).sum()  # Horizontal differences\n        ldiv += np.abs(patch[:-1, :] - patch[1:, :]).sum()  # Vertical differences\n        ldiv += np.abs(patch[:-1, :-1] - patch[1:, 1:]).sum()  # Diagonal (top-left to bottom-right)\n        ldiv += np.abs(patch[1:, :-1] - patch[:-1, 1:]).sum()  # Diagonal (top-right to bottom-left)\n    \n    return ldiv  # Return the accumulated diversity score","metadata":{"_uuid":"80966fbc-5c2d-4524-9acb-0809804a4177","_cell_guid":"071fc43f-b122-408e-9121-1991b86d4586","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T20:30:49.902795Z","iopub.execute_input":"2024-12-04T20:30:49.903422Z","iopub.status.idle":"2024-12-04T20:30:49.908593Z","shell.execute_reply.started":"2024-12-04T20:30:49.903392Z","shell.execute_reply":"2024-12-04T20:30:49.907688Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def plot_images(images):\n    num_images = len(images)  \n    plt.figure(figsize=(15, 5))\n    \n    for i, image in enumerate(images): \n        plt.subplot(1, num_images, i+1)\n        plt.imshow(image) \n        plt.axis('off')\n    \n    plt.show","metadata":{"_uuid":"6a0faafd-d45e-471a-b0f6-36f8cd4eba46","_cell_guid":"1c2f2e5b-a6f1-4a6e-88cd-198a3b2ae419","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-04T20:30:50.488242Z","iopub.execute_input":"2024-12-04T20:30:50.489044Z","iopub.status.idle":"2024-12-04T20:30:50.493453Z","shell.execute_reply.started":"2024-12-04T20:30:50.488997Z","shell.execute_reply":"2024-12-04T20:30:50.492603Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def smash_reconstruction(image, patch_size=8, rich_ratio=0.5):\n    \n    patches = normal_patches(image, patch_size)\n    patches.sort(key=texture_diversity, reverse=True)\n    \n    # Split patches into rich and poor texture based on sorted texture diversity\n    num_rich = int(len(patches) * rich_ratio)\n    rich_texture = [patches[i] for i in range(num_rich)]\n    poor_texture = [patches[i] for i in range(num_rich, len(patches))]\n    \n    rich_image = reconstruct(rich_texture)\n    poor_image = reconstruct(poor_texture)\n    \n    return rich_image, poor_image","metadata":{"_uuid":"9135b1c9-868d-471e-a5ed-497d2d8d9f0c","_cell_guid":"8e9aa0df-4356-4c42-97ee-4d92c9b2559c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-04T20:30:50.927744Z","iopub.execute_input":"2024-12-04T20:30:50.928461Z","iopub.status.idle":"2024-12-04T20:30:50.933596Z","shell.execute_reply.started":"2024-12-04T20:30:50.928426Z","shell.execute_reply":"2024-12-04T20:30:50.932696Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def high_pass(image):\n    filtered_images = []  # Initialize a list to store the filtered images\n    \n    for kernel in filters:  # Loop through each kernel in the filters list\n        # Split the image into B, G, R channels\n        b_channel, g_channel, r_channel = cv2.split(image)\n        \n        # Apply the filter to each channel\n        b_filtered = cv2.filter2D(b_channel, -1, kernel)\n        g_filtered = cv2.filter2D(g_channel, -1, kernel)\n        r_filtered = cv2.filter2D(r_channel, -1, kernel)\n        \n        # Merge the filtered channels back into a single image\n        filtered_image = cv2.merge([b_filtered, g_filtered, r_filtered])\n        \n        # Add the filtered image to the list\n        filtered_images.append(filtered_image)\n\n    return filtered_images  # Return the list of filtered images","metadata":{"_uuid":"d23d0acf-8075-4844-9029-7df8db0612d1","_cell_guid":"cef53f2e-31ef-447f-b076-77b019324432","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-04T20:30:51.418819Z","iopub.execute_input":"2024-12-04T20:30:51.419136Z","iopub.status.idle":"2024-12-04T20:30:51.424261Z","shell.execute_reply.started":"2024-12-04T20:30:51.419108Z","shell.execute_reply":"2024-12-04T20:30:51.423288Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def concat_high_pass(filtered_images):\n    return np.concatenate(np.array(filtered_images), axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T20:30:52.138595Z","iopub.execute_input":"2024-12-04T20:30:52.139381Z","iopub.status.idle":"2024-12-04T20:30:52.143441Z","shell.execute_reply.started":"2024-12-04T20:30:52.139349Z","shell.execute_reply":"2024-12-04T20:30:52.142633Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def plot_images(image, filtered_images):\n    # Set up the figure for a row of subplots\n    num_images = len(filtered_images) + 1  # Original image + filtered images\n    plt.figure(figsize=(15, 5))\n    \n    # Plot the original image\n    plt.subplot(1, num_images, 1)\n    plt.title(\"Original\")\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) \n    plt.axis('off')\n    \n    # Plot each filtered image in a row\n    for i, filtered_image in enumerate(filtered_images, start=2):\n        plt.subplot(1, num_images, i)\n        plt.imshow(cv2.cvtColor(filtered_image, cv2.COLOR_BGR2RGB)) \n        plt.axis('off')\n\n    plt.show()","metadata":{"_uuid":"a7786cd2-0e7c-4158-a3b7-9c67222a262d","_cell_guid":"0097af2b-7e5a-42a9-b323-23177500bf62","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-04T20:30:53.088409Z","iopub.execute_input":"2024-12-04T20:30:53.088748Z","iopub.status.idle":"2024-12-04T20:30:53.094113Z","shell.execute_reply.started":"2024-12-04T20:30:53.088718Z","shell.execute_reply":"2024-12-04T20:30:53.093151Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# **Label Mapping and Data Processing**","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nfrom PIL import Image\n\nsource_dir = \"/kaggle/input/tiny-genimage\"\ndestination_dir = \"/kaggle/working/GenImage\"\n\ndef create_target_structure(base_dir):\n    for split in [\"train\", \"val\"]:\n        for category in [\"REAL\", \"FAKE\"]:\n            os.makedirs(os.path.join(base_dir, split, category), exist_ok=True)\n\ndef move_and_resize_images(src_dir, dest_dir, target_size=(32, 32)): #To maintain a consistent file structure across datasets\n    create_target_structure(dest_dir)\n    \n    for root, dirs, files in os.walk(src_dir):\n        for file in files:\n            if file.endswith(('.png', '.jpg', '.jpeg')):  \n                full_path = os.path.join(root, file)\n\n                if \"ai\" in root.lower():\n                    label = \"FAKE\"\n                elif \"nature\" in root.lower():\n                    label = \"REAL\"\n                else:\n                    continue  \n\n                split = \"train\" if \"train\" in root.lower() else \"val\"\n\n                dest_path = os.path.join(dest_dir, split, label, file)\n\n                # Resize and move the image\n                try:\n                    with Image.open(full_path) as img:\n                        img = img.resize(target_size, Image.BILINEAR)\n                        img.save(dest_path)\n                except Exception as e:\n                    print(f\"Error processing {full_path}: {e}\")\n\nmove_and_resize_images(source_dir, destination_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T20:26:43.551999Z","iopub.execute_input":"2024-12-04T20:26:43.552809Z","iopub.status.idle":"2024-12-04T20:29:43.343732Z","shell.execute_reply.started":"2024-12-04T20:26:43.552777Z","shell.execute_reply":"2024-12-04T20:29:43.343032Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"train_real_path = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train/REAL\"\ntrain_fake_path = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train/FAKE\"\ntrain_real_li = [(os.path.join(train_real_path, filename), 0) for filename in os.listdir(train_real_path) if filename.endswith(('.jpg', '.png', '.jpeg'))]\ntrain_fake_li = [(os.path.join(train_fake_path, filename), 1) for filename in os.listdir(train_fake_path) if filename.endswith(('.jpg', '.png', '.jpeg'))]\ntrain_real_path = \"/kaggle/working/GenImage/train/REAL\"\ntrain_fake_path = \"/kaggle/working/GenImage/train/FAKE\"\ntrain_real_li += [(os.path.join(train_real_path, filename), 0) for filename in os.listdir(train_real_path) if filename.endswith(('.jpg', '.png', '.jpeg'))]\ntrain_fake_li += [(os.path.join(train_fake_path, filename), 1) for filename in os.listdir(train_fake_path) if filename.endswith(('.jpg', '.png', '.jpeg'))]\ntrain_li = train_real_li + train_fake_li #training images","metadata":{"_uuid":"a581097b-0ddf-4e30-abde-560254a26401","_cell_guid":"0d09762c-7a28-46df-bfc5-53e288f9f8e6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-04T20:30:57.456629Z","iopub.execute_input":"2024-12-04T20:30:57.457353Z","iopub.status.idle":"2024-12-04T20:30:57.729848Z","shell.execute_reply.started":"2024-12-04T20:30:57.457315Z","shell.execute_reply":"2024-12-04T20:30:57.728781Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"test_real_path = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test/REAL\"\ntest_fake_path = \"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/test/FAKE\"\ntest_real_li = [(os.path.join(test_real_path, filename), 0) for filename in os.listdir(test_real_path) if filename.endswith(('.jpg', '.png', '.jpeg'))]\ntest_fake_li = [(os.path.join(test_fake_path, filename), 1) for filename in os.listdir(test_fake_path) if filename.endswith(('.jpg', '.png', '.jpeg'))]\ntest_real_path = \"/kaggle/working/GenImage/val/REAL\"\ntest_fake_path = \"/kaggle/working/GenImage/val/FAKE\"\ntest_real_li += [(os.path.join(test_real_path, filename), 0) for filename in os.listdir(test_real_path) if filename.endswith(('.jpg', '.png', '.jpeg'))]\ntest_fake_li += [(os.path.join(test_fake_path, filename), 1) for filename in os.listdir(test_fake_path) if filename.endswith(('.jpg', '.png', '.jpeg'))]\ntest_li = test_real_li + test_fake_li #testing images","metadata":{"execution":{"iopub.status.busy":"2024-12-04T20:30:57.932133Z","iopub.execute_input":"2024-12-04T20:30:57.932436Z","iopub.status.idle":"2024-12-04T20:30:57.972950Z","shell.execute_reply.started":"2024-12-04T20:30:57.932408Z","shell.execute_reply":"2024-12-04T20:30:57.972351Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"image = cv2.imread(\"/kaggle/input/cifake-real-and-ai-generated-synthetic-images/train/FAKE/1000 (10).jpg\")\nrich_image, poor_image = smash_reconstruction(image)\nfiltered_images = high_pass(rich_image)\nplot_images(rich_image, [poor_image, filtered_images[7]])","metadata":{"_uuid":"befa1f4b-3edb-43cd-81c1-ba4d3bc7938b","_cell_guid":"ae78f4fd-33a5-4660-b5ab-df614da1d36d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-04T20:30:58.518000Z","iopub.execute_input":"2024-12-04T20:30:58.518293Z","iopub.status.idle":"2024-12-04T20:30:58.740835Z","shell.execute_reply.started":"2024-12-04T20:30:58.518267Z","shell.execute_reply":"2024-12-04T20:30:58.740043Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 3 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAABAkAAAGrCAYAAACi687DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsQElEQVR4nO3daZSdB33n+f+turUvUmm1JcuyZMkyXrEd1gaDmywNuBNC4kwI8cQ06cDJnCwnE2bS5GQOSafJHJIzcE7OyRmSMRCaJZA0JCQsCYGAY4zBwdjyKtmW5U22JZWWKtV+6955wY0DHTrUz+2Kbfh83iF/66lH9xb3uc9P13Kj0+l0CgAAAPie1/N0nwAAAADwzGAkAAAAAKrKSAAAAAB0GQkAAACAqjISAAAAAF1GAgAAAKCqjAQAAABAl5EAAAAAqCojAQAAANBlJHgave1tb6tGo/GkvvZ973tfNRqNOnjw4FN7Ut/k4MGD1Wg06n3ve9+qfQ8AAACeOYwET9Idd9xRP/3TP11bt26tgYGB2rJlS73+9a+vO+644+k+NQAAAHhSGp1Op/N0n8Szzcc+9rF63eteV+vWras3vvGNtWPHjjp48GBde+21NTk5WX/yJ39SP/qjP/odj9NqtarVatXg4GB8DsvLy7W0tFQDAwNP+tMI38nBgwdrx44d9d73vreuueaaVfkeAAAAPHM0n+4TeLa577776uqrr66dO3fWddddVxs3bnzin/3SL/1SvfSlL62rr7669u7dWzt37vy2x5iZmamRkZFqNpvVbD65p6C3t7d6e3uf1NcCAADAt+NfNwj97u/+bs3OztYf/uEffstAUFW1YcOGeve7310zMzP1jne8o6r+6e8duPPOO+unfuqnamJiol7ykpd8yz/7ZnNzc/WLv/iLtWHDhhobG6sf/uEfrkceeaQajUa97W1ve6L7dn8nwVlnnVVXXnllXX/99fX85z+/BgcHa+fOnfX+97//W77HsWPH6ld/9VfrwgsvrNHR0RofH69XvvKVdeuttz6FjxQAAADPNj5JEPrLv/zLOuuss+qlL33pt/3nl19+eZ111ln1yU9+8lt+/aqrrqrdu3fX29/+9vqX/g2Pa665pj760Y/W1VdfXS984Qvri1/8Yr361a9e8fnde++99eM//uP1xje+sX7mZ36m3vOe99Q111xTl112WZ1//vlVVXXgwIH68z//87rqqqtqx44d9fjjj9e73/3uetnLXlZ33nlnbdmyZcXfDwAAgO8eRoLAyZMn69ChQ/UjP/Ij/2J30UUX1Sc+8Ymanp5+4tcuvvji+tCHPvQvft3NN99cH/3oR+uXf/mX653vfGdVVf38z/98veENb1jxn/Lv27evrrvuuidGjJ/4iZ+obdu21Xvf+976vd/7vaqquvDCC2v//v3V0/NPHyS5+uqr69xzz61rr722fuM3fmNF3wsAAIDvLv51g8A/3vSPjY39i90//vOpqaknfu3Nb37zdzz+Zz7zmar6xjDwzX7hF35hxed43nnnfcunHDZu3Fh79uypAwcOPPFrAwMDTwwEy8vLNTk5WaOjo7Vnz566+eabV/y9AAAA+O5iJAj8483/N39C4Nv5dmPCjh07vuPxH3jggerp6fln7a5du1Z8jmeeeeY/+7WJiYk6fvz4E/+73W7XO9/5ztq9e3cNDAzUhg0bauPGjbV37946efLkir8XAAAA312MBIE1a9bU6aefXnv37v0Xu71799bWrVtrfHz8iV8bGhpa7dOrqvof/hcPvvnvQXj7299ev/Irv1KXX355feADH6i//uu/rs9+9rN1/vnnV7vd/lc5TwAAAJ55/J0EoSuvvLL+6I/+qK6//von/isF3+zv//7v6+DBg/WmN70pPvb27dur3W7X/fffX7t3737i1++9997/qXP+7/3Zn/1ZXXHFFXXttdd+y6+fOHGiNmzY8JR+LwAAAJ49fJIg9Ja3vKWGhobqTW96U01OTn7LPzt27Fi9+c1vruHh4XrLW94SH/uHfuiHqqrqD/7gD77l13//93//yZ/wt9Hb2/vP/gsLf/qnf1qPPPLIU/p9AAAAeHbxSYLQ7t2764//+I/r9a9/fV144YX1xje+sXbs2FEHDx6sa6+9to4ePVof/vCH6+yzz46Pfdlll9WP/diP1bve9a6anJx84j+BuH///qqqajQaT8nv4corr6zf+q3fqje84Q314he/uG677bb64Ac/WDt37nxKjg8AAMCzk5HgSbjqqqvq3HPPrd/5nd95YhhYv359XXHFFfXWt761Lrjggid97Pe///112mmn1Yc//OH6+Mc/Xt///d9fH/nIR2rPnj01ODj4lJz/W9/61pqZmakPfehD9ZGPfKQuvfTS+uQnP1m/9mu/9pQcHwAAgGenRue//9w5zzi33HJLXXLJJfWBD3ygXv/61z/dpwMAAMB3KX8nwTPM3NzcP/u1d73rXdXT01OXX37503BGAAAAfK/wrxs8w7zjHe+or33ta3XFFVdUs9msT3/60/XpT3+6fu7nfq62bdv2dJ8eAAAA38X86wbPMJ/97GfrN3/zN+vOO++sU6dO1ZlnnllXX311/fqv/3o1mzYdAAAAVo+RAAAAAKgqfycBAAAA0GUkAAAAAKrKSAAAAAB0rfhvwnvBntOiA7fb7ahvZXlVT2+UN/sHo350bDzqx9dORP3gyHDU33frTVFfVdVuZH2jsse00Zv9RYrpczA0NBT1wyOjUT8wlD0HH/vk56IeyPQ3s//P9/RkO3draT7qx0dGov7MrVui/oytp0f9c/acG/U7d+2M+v/tV98a9VVVW07P3hvs2HZG1G/fnv1XfTavXxf1CwvZz8Tk0cNRf+jQw1H/xX/YG/VArtFI3yD3RXn/QPb+uDd8P91T2U3T4uJi1C8tLUX9v47wL29vhveJPdnPRKOzHPWt5ew567Sz46dW8lcS+iQBAAAAUFVGAgAAAKDLSAAAAABUlZEAAAAA6DISAAAAAFVlJAAAAAC6jAQAAABAVRkJAAAAgC4jAQAAAFBVRgIAAACgy0gAAAAAVFVVc6Xh3MJSdOCenmx/aDb7o763b8Wn3u37or7TXo76uZlTUb+4OB/1MwutqK+q6u3tjfq+vkbU9zSy46darez3PDc3F/VLS9nPNLC6hocHo37z5o3ZN2hl/58fGh6I+tM2rAuPn13H9t2zN+r/4i8/GvU/+8Zror6q6ujhx6L+yzdcH/W33PyVqF8zPhz1mzauz44/Ohr1d++7O+qB1TfQl71/XQzfjy7Op+8vs3umgcHs2jQ8tibqe5rZ+Rw/fDjqhwaza19VVXhbVq129pw1Oul9a/Z7SN9P9IT3cDPTU1G/onN4yo8IAAAAPCsZCQAAAICqMhIAAAAAXUYCAAAAoKqMBAAAAECXkQAAAACoKiMBAAAA0GUkAAAAAKrKSAAAAAB0GQkAAACAqjISAAAAAF3NlYbzrU504J6erO/ttMK+EfXN9mLUdyo7fjWyvaWvkz0+rc6Kn6onLC+n32M56nuXs8d0qZUdf6CdnX/6nHXC5wBYXW//7bdF/U033RT1n/iLj0V969H5qD98qD/qR4YHon7LaZui/tSpk1G/Z/fZUV9VdezoY1E/MzUV9eNrhqO+v5ldi4+fOBz1mzaMZf3mdVEPrL7e8C3+yED22l49q3sP0dOT9e2lhahfXMjer6fareyer6qq2ezL+r6sTx/T+DloZ4/p4lJ2j9V6Eo/pd+KTBAAAAEBVGQkAAACALiMBAAAAUFVGAgAAAKDLSAAAAABUlZEAAAAA6DISAAAAAFVlJAAAAAC6jAQAAABAVRkJAAAAgC4jAQAAAFBVVc2Vhp2e/ujAnXB+aDeyL+i0O1m/tBT17XY76lvL2fGbzRU/9FVV1enN+qqqRk/2mPb2NrLjN7K+08ke04WFhahfai1GfU/4+ACra3b2VNT3D/RG/Y7t26J+3brxqD918njUHzt+NOo3bJyI+oVW9hqaXgOqqs7YtjXqn3Pe7qgf6M+e42OHH436nvC9x/Mve27Utxaz5wBYfQuLy1HfaIR9/P47e51r9g9kfTM7fm9Pds8xV7NRv9SK8u7XhPdxlfXpHUH/4GDW92f30QP92fGrk1+/vxN3SQAAAEBVGQkAAACALiMBAAAAUFVGAgAAAKDLSAAAAABUlZEAAAAA6DISAAAAAFVlJAAAAAC6jAQAAABAVRkJAAAAgC4jAQAAAFBVVY1Op9NZSfgLv/Tz0YFXeNgnrF+7PuqPHDkS9aMDQ1G/sLAQ9QPNgahfruzxaY72RX1VVaPRyL5Hoxn18/PzUT8+NBL1rVYr6ms5y/sGs+fsN/7zb2bfAIhs2TgR9c973vOj/tw950X90lL2onL4yGTUz8zPRX2r1Y76+fA69nef+0zUV1UNDw1H/ejIeNSn14FWJ3uMms3supeez8zMTNQvLGXXVSCXvj+G7zYruU/3SQIAAACgqowEAAAAQJeRAAAAAKgqIwEAAADQZSQAAAAAqspIAAAAAHQZCQAAAICqMhIAAAAAXUYCAAAAoKqMBAAAAECXkQAAAACoqqrmSsPTT98aHbjdbkf9sSNHo358fDzq1wxn/fLyctSfOjkV9aNjo1Ffg31ZX1Xt1lLU33/gQNRv25L9TKxduy7q+/qy3/PU8RNRPzY2FvXA6jo1sxj1f/M3X4z6T376b6M+1dvojfrBof5VOpNvWFycj/oztm6Jv8fycifqO8vZe4P+vuwx7ctOp9rtVtQPDGZ/tjIxvj7qAfift3Hjxqg/Gd5DLIb3WM9GPkkAAAAAVJWRAAAAAOgyEgAAAABVZSQAAAAAuowEAAAAQFUZCQAAAIAuIwEAAABQVUYCAAAAoMtIAAAAAFSVkQAAAADoMhIAAAAAVVXVXGnY1zsYHXhxeT7qjx45FvU7zzor6oeHh6N+oNkX9WuGR6O+2Z8d/0Mf/1jUPxnHjk5G/fTu6ag/sv5o1G9ctz7qz9m1O+qb/b1RD6yuV73yB6K+025EfW+jE/U9PdmO3motRf3i4mLUN/uy8xkczK7b6yY2RH1V1fx8dq1vLWaPUaMne86azRW/relqh3l2PhWeP/Dst2XraVE/eTi7B1pYyq4dzzSveMXL4q+5/8CBqL/4ooui/vjxU1F/Yupk1N93331RPz2d3WOtBp8kAAAAAKrKSAAAAAB0GQkAAACAqjISAAAAAF1GAgAAAKCqjAQAAABAl5EAAAAAqCojAQAAANBlJAAAAACqykgAAAAAdBkJAAAAgKqqaq40vG3vHdmRG+0oX1hYivqD9z8Y9XfN3hn1Q/0DUf/Kf/eDUV+NRpQ/cPCh7PhVNTw0FPVrRkei/vFHH4v6A/vuifq1a8ej/vsuuyTqBwf7ox5YXVs2bYj6drsV9a3F7DrT7mTHbzaz19xmszfqG+1O1C8uZ+c/d+pk1FdVjY6ORn3/muwxWlxcjPrWwkLU9/Rkf1bS6Mmeg04n64HVt+fcs6N+3933Rf2a0bGo37xxU9SvHZ+I+qnp6aj/2tdvjvrUvfcdiL/m4YcejvqFpez6N7F2fdSfddZZUX/pcy+L+ve879qoXw0+SQAAAABUlZEAAAAA6DISAAAAAFVlJAAAAAC6jAQAAABAVRkJAAAAgC4jAQAAAFBVRgIAAACgy0gAAAAAVJWRAAAAAOgyEgAAAABVVdXodDqdlYRv+cW3RAdeWm5FfU9Ptlf09fVFfbudnc/i4mLU9/b2Rv3Q0FB2/PDxqapqdxpRv7y8nB0/fEzT5zjt2+121LeWs+f4d9/5/0Q9kLnuc38T9SdOnoz6I0eORP2n/jo7nwvOvyjq7757f9QfnTwe9dddd13Uf+Jj/zXqq6q++tWvRv1dd+2L+kcOPRr1V3z/D0T95NHsMT344ENR/6lPfSrqO53sOgzkGo3s/fGz3Yufd2nUL4fvp7/ytVui/nvR8557YdQPDw9GfWspyuv6FVy7fZIAAAAAqCojAQAAANBlJAAAAACqykgAAAAAdBkJAAAAgKoyEgAAAABdRgIAAACgqowEAAAAQJeRAAAAAKgqIwEAAADQZSQAAAAAqqqqudJw7cR4dODe5ooPXVVVy8vLUb/UWoj6vr7hqB8ezvp2ux31p05NRf3QQHY+VVV9A/1R32g0or7VWlzV4/f19UV9Kj1/YHV9+IPvifrrv3xD1D/66ONRv2nz6VHf19cb9QuL2WtQs7cT9cND2Wvof/q1X476qqoDBx6J+r6B7DG6/PKXR/3BA/dE/WIre+9xzu4dUb/2J/+XqAd4qt1w081P9yl8z7vpltue7lOI+SQBAAAAUFVGAgAAAKDLSAAAAABUlZEAAAAA6DISAAAAAFVlJAAAAAC6jAQAAABAVRkJAAAAgC4jAQAAAFBVRgIAAACgy0gAAAAAVFVVc6Xh2NhQdOD+/v7sRJorPpWqqurUctQvLi5G/fJydvz+vt6oHx3ZEPWDfQNRX1XVHOiL+kajEfXtdntVj98XPqadTifq058JYHV97vOfifrDh09G/eia7Dr2/a94WdQPDo1F/cBAdj7nnndh1F/5718V9b/7O/9H1FdVjY5lr+vN8L3BwED23mBpaS7q166ZiPoXvOB5UT81dSrqAeCZwCcJAAAAgKoyEgAAAABdRgIAAACgqowEAAAAQJeRAAAAAKgqIwEAAADQZSQAAAAAqspIAAAAAHQZCQAAAICqMhIAAAAAXUYCAAAAoKqqmisNT9u0PjpwT1+2P0xNTUX9sWPHon5sbCzqt55xetSn53PXXXdF/Quf98Kor6pqN9pRf+DAgajfv39/1K9duzbqt2/fFvX33HNP1P/VX/1V1F/9H94c9UBm0+bsOrNr946oPzE1E/W33X5L1M/OLUb9hvWnRf0F518Y9WvHRqP+Na95TdRXVQ0PD0f97Xdm14279t0d9a2lTtRv3Zb9DP1gf3/UDwz0RT3wvWdoKHutnps7tUpn8uRccO7FUX/73beu0pn8k61btkf9nj17on5mZjrqDx58MOq3b8/O/7WvfW3Ur4RPEgAAAABVZSQAAAAAuowEAAAAQFUZCQAAAIAuIwEAAABQVUYCAAAAoMtIAAAAAFSVkQAAAADoMhIAAAAAVWUkAAAAALqMBAAAAEBVVTU6nU5nJeGPveaV0YFPnToV9YcPH436xx9/POpPnDgR9a3Wih6WJwz0NbJ+YCDqX/7yl0d9VVWrvRz1s7PzUX/y5MmoP3bsWNSnz9nsbPYz12pFeS2EPxNA5v/6P38p6k+cyF5TenqyXXxubi7q161bF/X3339/1O/bty/qHz/8WNRfeN7FUV9VVT3Z6+LatWujPn0O+vv7o/6hhx6K+ulTU1HfCi80+w9m74WA3E/95Ouivt1uR32zmV1rdu3aFfV7zt0d9VNTx6P+lr13Rv0lF10S9Z//4uejvqrqwYMPRP3DDz8Y9du3b4/6Sy7JrpdHjhyJ+s9//gtR/5rXXBn17/7D93/HxicJAAAAgKoyEgAAAABdRgIAAACgqowEAAAAQJeRAAAAAKgqIwEAAADQZSQAAAAAqspIAAAAAHQZCQAAAICqMhIAAAAAXUYCAAAAoKqqGp1Op7OS8HWvfVV24GZv1Pc0mlG/uLgY9SempqP+6NGjUT85ORn1M6dmo/4VL/83UV9V1Wxmj2l//2D2DXqyjWlpaSnqT83ORf3MzEzUL8xnP0PX3fCVqAcy2zeNRX2zvy/qJyYmoj69zvT390f99HR2XZpYtzbqBwez1/SvfPm2qK+qasdfsbpGhrO+tzd7r7Jjx46ob4R/FPOVr9+TfQEQazQaT/cpfFd5+cteEvVf+OL1q3QmrNRKbv99kgAAAACoKiMBAAAA0GUkAAAAAKrKSAAAAAB0GQkAAACAqjISAAAAAF1GAgAAAKCqjAQAAABAl5EAAAAAqCojAQAAANBlJAAAAACqqqq50nDqxPHswP19UT84OBz1A0ODUb9t/LSoP/OMrVHf6XSivt2O8tp/563ZF1RVf39/1A8Pj0b96PhY1I+MjET9po0TUd/YuCHrw+cMWF1jY9lryvz8YtQfPTwZ9T092Y4+OtqI+sH+gajftmVb1E9PT0f9eeduj/qqqmZf9nvo68veG6TPwdLSUtTPzMxE/dk7dkb95GT2MwesvjUbsveL7aXs/eL01Imor85y1j/DfOGL10f9WTvPfhLfJbsWzM7PRv3xyWNRv7QwF/XPRj5JAAAAAFSVkQAAAADoMhIAAAAAVWUkAAAAALqMBAAAAEBVGQkAAACALiMBAAAAUFVGAgAAAKDLSAAAAABUlZEAAAAA6DISAAAAAFVV1VxpePTI4ejAvX0rPnRVVQ0ODEX9yNho1I+NrYn6wcHBqO/pyfaWnupEfXXaWV9ViwvzUb+8vBz1S0sLUb+4uBj1IyMjUd/f3x/16XMGrK7lVivqR0bGoj59jWg2s+vY3NxM2M9F/eDgcNSfPHky6q++6lVRX5VfNxYXsud4cDi7DlQjvBY3s2v95S+7IuoPHXo06oHVNzQwEPV9w9m1Y3wiuzYtLWTvj09Nn4r69FrTWU7vObLrwKFHHgmPXzU8nN33DQ1nr+2bN2+K+oXF7B5rdibrlxaye6zlTnhfuQLukgAAAICqMhIAAAAAXUYCAAAAoKqMBAAAAECXkQAAAACoKiMBAAAA0GUkAAAAAKrKSAAAAAB0GQkAAACAqjISAAAAAF1GAgAAAKCqqporDc++4NLowOOjI1G/9+abov6BW78e9UN92R7SaS1G/Qued0nUP/bg/qi/8fbjUV9VdfEFz4n69tRU1O+7846oHx3sjfpNG9ZE/StednnU1/zjWQ+sqh/5vomo3xy+RpyxfVfUL7X7on7D5rOy43f6o77dm11XHz18IuqHp66P+qqqo4cORv34YPacrRk4I+qXe0aj/q67Ho36//LpT0b9A49ORv2rf/KaqAdyjz3yyKoef/367Fq26zkXR32nN7t29PYPRv2NX7056jvTC1F/6Z7s8amquv3ue6N+uW9D1O/YdX7Ud3qzx/TE1FzU3/UP6fV4fdh/Zz5JAAAAAFSVkQAAAADoMhIAAAAAVWUkAAAAALqMBAAAAEBVGQkAAACALiMBAAAAUFVGAgAAAKDLSAAAAABUlZEAAAAA6DISAAAAAFVlJAAAAAC6misN/9vH/jw68MUXPCfqx4aGov7k9GzUt4f6ov68PTujfufOXVF/6QVnR/3f7ftI1FdVLSwsRP3undk5NRqNqN+4bk3U9/d0ov6zX/j7qF+Ymoz6345qIPWqH/q3Ub/vzr1Rf/zI41F/+hm7o/7GL30p6r/w5Zuj/sFHT0Z9c2A86tf3nYj6qqqFU1l/xvb+qH/hSzZH/b333xX1Bx7KfiYePZL9hpuDo1EPrL43/K+vj/r/+v4PRv3k5PGo77vzH6J+anom6sfGs/ffnZMnoj51cmEi/pqFxXbUnzp8OOoPh33quedl94k/8MKsH9h0btSvhE8SAAAAAFVlJAAAAAC6jAQAAABAVRkJAAAAgC4jAQAAAFBVRgIAAACgy0gAAAAAVJWRAAAAAOgyEgAAAABVZSQAAAAAuowEAAAAQFVVNTqdTmcl4fjYUHTg8dGRqL/w/POivrexotN+wtlnnRn1hx4+GPVf+Lsbon5pIcprKsuflG1bN0X9urVrov6i8/ZE/T3790X9rbfeE/UT2Y9oPXwq+5kDMpdubkT9y17y3Kh/0UuuiPq160+L+r/81Oei/pbb9kf9gYcejfqFxSivwQovTFX146/NHtMLL7ok6h88dCTqr/vSV6N+8th01C+0sp/RRk8z6u9+4GDUA7nn7Dkn6jvLS1G/776DUb/aLj5nR9S/5tUvivrf/IOPRP1zztoe9VVVu3dk93HXX39j1PeGf2x+ZGo++4LQ7s1ZP9u7PuoffuTod2x8kgAAAACoKiMBAAAA0GUkAAAAAKrKSAAAAAB0GQkAAACAqjISAAAAAF1GAgAAAKCqjAQAAABAl5EAAAAAqCojAQAAANBlJAAAAACqqqq50nBwcCA68GKrFfUPPvxI1G89/bSo7xkcivrHJk9G/VJUV23YPBL1L7zw+8LvUHXsxImoP3HsWNR3GtnGtNxY8Y9bVVVNzS9Efd9glNfA+NrsC4BVdehU1t8/mfVHv7A36h859NmoHxpbG/Vjm7dH/Rn966L+2PHsOjY3fSjqq6o2nnVp1F/4on8X9VvD38OlL/33Uf+lL98Y9fc/8FDU+7MYeOY5cOD+qL/w/POifse2LVE/ONAf9XfdezDqb92f/X7TPnXXvgOr/jUjQ9lNwa5t26L+yNQ9UZ969GjWn1oO3xCtgKsXAAAAUFVGAgAAAKDLSAAAAABUlZEAAAAA6DISAAAAAFVlJAAAAAC6jAQAAABAVRkJAAAAgC4jAQAAAFBVRgIAAACgy0gAAAAAVFVVo9PpdFYS/u8/+4bowJde9ryov/3226P+E5/486jvbTaivt1uRf2jhw5Hfeq8c86Lv2ZkzZqoX15ejvpWaynqT548HvXzc9NZPzsT9cPDg1F/+yPZ+QOZ83adFvWtVvY6vbCwEPVV7ageHR2N+vHx8ahvNLLr2Pz8bNSPjGTXjKqqmZnsdXd29lTUp9eldjt7zlb4FugJAwN9UT88PBz1X7vtoagHculr6Wr7wde8Kuq/8qUvR/3JI9n714n1I1G/68Lvi/qbvnpD1FdV1Wx2z5Haumt71D9y8IHsG2RvV6oxlN2jdObms34F1z6fJAAAAACqykgAAAAAdBkJAAAAgKoyEgAAAABdRgIAAACgqowEAAAAQJeRAAAAAKgqIwEAAADQZSQAAAAAqspIAAAAAHQZCQAAAICqqmp0Op3OSsKz1q6NDtw/PBr127dvi/qzd+2M+rvv3Bv1IyODUb+4MBf1hw8/lh2/NRT1VVUzc/NRPzjYH/XnnLM76m+/7etRv/Os7Geit9mI+tlT01H/pdsORj2Q+cGXf1/UP/7441F/4sSxqG80steUdrsd9a1WK+pHR4ejfsOGDVE/MDAS9VVVhw8fjvqTJ49HfV9fX9QPDWXXyp6e7M9K+vp6o35wMHsv8eWv3Rf1QO7t//fvRP3NX70x6v/bxz8R9c9248Obo35ifXa/UVU12J9dC06dmon6R8L3E8Ph+cwuLkX9alvJ7b9PEgAAAABVZSQAAAAAuowEAAAAQFUZCQAAAIAuIwEAAABQVUYCAAAAoMtIAAAAAFSVkQAAAADoMhIAAAAAVWUkAAAAALqMBAAAAEBVVTU6nU5nJeH2TadHB56bm4v6LVtOi/qrf/p1UX9yajLqT02fiPqDB/ZH/fDwYNTfePODUV9Vtbg4H/V9fb1Rf/lLXhT1o2PZ73lsdCjq52anon7Tpg1R/59+6/ejHshcuHtL1DebzWdYv7q7+/x89po+Ozsb9UcOH4v6qqrTTt8c9Rs3boz6np7sMZ2fz37Pq/2Ypv39j56KeiA32Je9tp977jlRPzQ8HPU3fvVrUf+9aLDZiPodO3ZG/V333Bf1z3Yruf33SQIAAACgqowEAAAAQJeRAAAAAKgqIwEAAADQZSQAAAAAqspIAAAAAHQZCQAAAICqMhIAAAAAXUYCAAAAoKqMBAAAAECXkQAAAACoqqrmSsOxiXXRgSc2ZfvDyMhQ1F/35S9F/WB/lNf2M7dG/XOfd1nULy7OR31n6Iyor6q65559UT8WPgdTczNRf+jwg1E/sWY06s97zp6o7+1b8Y8/8K9gcCB7DUo1Glm/uLgY9dPTC1E/NzcX9en5tNvtqP/Z//Afo76q6o477oj6/ffcHfWzs7NRn1pYyK7FS0vLUd/jj2LgGWehlf3/+Nbb74r611756qi/+Pxzo/7o5Imof+Sxx6L+mWi+1Yn6mZnsHmXLpvVRf/rp2X3ZoUcPRf3RycmoX1rOrvcr4fIFAAAAVJWRAAAAAOgyEgAAAABVZSQAAAAAuowEAAAAQFUZCQAAAIAuIwEAAABQVUYCAAAAoMtIAAAAAFSVkQAAAADoMhIAAAAAVVXVXGm4buOG6MCnZk5E/fHjk1F/4uSRqJ9YOxr1rVqK+k3r10X98PBw1P/FX3w86quqWsuLUd/s6UT9xMSaqN92xmlRPzo6EvUHH3go6ptNGxk8k1z+Az8S9X19fVHfruWoX1zMXkPblb2G9vb2ZsdvtKM+Pf/+NZuivqrqkhdnX3PRi14a9e129ntu9Gav6z09Wd/qtLK+lfXAM89Q/0DU33b33VE/EL4f3X7mGVG/IbxHefjRQ1E/eexE1J933nlRX1U1MZ7dc4yuGY/6hx48EPVHJg9H/aZN66N+zUR2/qdOzUf9SrhLAgAAAKrKSAAAAAB0GQkAAACAqjISAAAAAF1GAgAAAKCqjAQAAABAl5EAAAAAqCojAQAAANBlJAAAAACqykgAAAAAdBkJAAAAgKqqaq40vPGG66IDtzvZifT1Zv2GDRNRv2ZkMOqX5pei/vDhY1G/vPx41J9z9plRX1W1tLQQ9e12O+qHBvqifrAvew4WZuej/uj8YtR3Kvv9Aqtr/fr1Ub+0lL1OL3ey/8+PjIxFfU+zEfWdTnahTF+jU+vXZI9/VVWr1Yr6RiN7jHqa2ZuDTmc56heWsutGqtlc8dss4F/JBRdfFPWLs6eivhW+Dk3PZ+93J2emo74nvCkbGx+N+sljJ6L+zjvvjPp/Df1D/VE/MjIc9fNHsmtNT/jH+MvZpXhl5/DUHxIAAAB4NjISAAAAAFVlJAAAAAC6jAQAAABAVRkJAAAAgC4jAQAAAFBVRgIAAACgy0gAAAAAVJWRAAAAAOgyEgAAAABVZSQAAAAAuporDX/0h38oOnCn04n6dru9qn2qt7d3VfuB/sGov+yiTVFfVdXbXPHT+40+/D0sLy9H/cLCUtYvLUZ9+jPXaDSiHlhdE2vWZF/Qk+3cPWG/3MmuM7Ozs6vap7t+M7wGDPb3R31V1cDYWNSnV+7p6emoP3b85Koef3ZuIeqXlrLrHrD6br9179N9CjzDLM5l9xxp/2zkkwQAAABAVRkJAAAAgC4jAQAAAFBVRgIAAACgy0gAAAAAVJWRAAAAAOgyEgAAAABVZSQAAAAAuowEAAAAQFUZCQAAAIAuIwEAAABQVVXNlYabJ9ZmR+7N9oeent6oby0vR/3CwtKq9kvLrahvtztRPzLUH/VVVcvhY7TUyn4PnU72e2g2s99D2nfCzSs9f2B1nX765qjv789eI2bmFqL+sccei/qpqVNhPxX1Rw5PRv3jjz8e9W/+jz8T9VVVvb3ZtfvkyZNRf/To0VXtT05nz9kDBx+K+nvvvTfqAZ5ujUbWjw4NRP30bHYtrqqqxopvWbuye6ByT/DP+CQBAAAAUFVGAgAAAKDLSAAAAABUlZEAAAAA6DISAAAAAFVlJAAAAAC6jAQAAABAVRkJAAAAgC4jAQAAAFBVRgIAAACgy0gAAAAAVFVVc6Xhhq2nRQc+Nnk86m+6+etRPzI6HvWPh+fTWmpH/Y9d9eNRP9g/EPUP33dX1FdVjYxlj9Hhhw9F/S17b4v68y+6JOrXrlsf9Z/73OeifnCwP+qB1fW3f/v5qJ+YmIj66enpqF9cXIz6kZGhqB8bGoz6pbHsunHvvsNRf+ut2Wt6VVVvb2/Uz8zMRH1fX3b8teOjUb+8tBD1G9dlx59aPxL1wLPfJRdcEPXHJiejfu1o9jrU6cnuafbuuy/qp2ez19HsVf0bljutqF87kl1fL7rwuVF/x97svvV4+Bj92xe/MOoXFpaifiV8kgAAAACoKiMBAAAA0GUkAAAAAKrKSAAAAAB0GQkAAACAqjISAAAAAF1GAgAAAKCqjAQAAABAl5EAAAAAqCojAQAAANBlJAAAAACqqqq50vArN3wlOvCWM7ZG/caNG6P+rz75+ajfuu30qD9j2/ao/8+/9dtRv2v32VHfOvF41Fflz0E1ss2op7Ec9X/z6b+K+tn57PibT9sU9WvGRqMeWF0bN66P+kajEfXr109E/cTEmqg/fPhw1N9001dX9fizczNRv3lz9vhXVR05ciTqZ2enon5sPHudnp6ajvq77r496k+cOBH1i0vzUQ+svt17zon6e/btj/qv3569rpx/8XlRf9+BR6L+BS98XtT/l6uvifovfCZ7f79m686or6q68cabov7hB+6N+iMzrah/xY/+ZNR/+UtfiPqv3nlX1E+dOBn1K+GTBAAAAEBVGQkAAACALiMBAAAAUFVGAgAAAKDLSAAAAABUlZEAAAAA6DISAAAAAFVlJAAAAAC6jAQAAABAVRkJAAAAgC4jAQAAAFBVVY1Op9NZSfjcXaet6oksdxpRf/au3VF//0OHov7Bhx+J+l3n7In6++67L+ov3nl61FdVzc7ORv3cwmLU9/YNRP3w2Jqob/YPRv2xEyej/uCDD0X99Kn5qAcyf/z//b9Rf8stt0T9DTfcEPWdznLUr127Nurb7XbUt1qtqD9+YjLqX/D8F0X9k/ke+/fvj/q5ubmoHxjIrktDQ+F1bHg06qens+vS1269N+qB3PP/zUuj/six41F/8O47oj41OrEx6k8dP7JKZ/INa/qz/uRi/mfUzZHstbc1H75nX87ugVZbo5ldm0aGsnum6akT37HxSQIAAACgqowEAAAAQJeRAAAAAKgqIwEAAADQZSQAAAAAqspIAAAAAHQZCQAAAICqMhIAAAAAXUYCAAAAoKqMBAAAAECXkQAAAACoqqpGp9PprCQ8Z9NAdOCh0ZGoHx9bG/XTM7NRP7Fhc9SvXbc+6vcfuD/qZ2bmon7TUDvqv6ER1T3N/qjv7cv6TqM36sfXrI36kbVZPze/GPWf+usvRD2Qec7uM6J+/fqJqG+1WlG/vLwc9QsLC1HfbmfHHxsbj/qFhfmoX1rKHp+qqr6+ZtRPT0+Hx++L+uHh4VU9/sBA9l7o+InJqL/19oeiHsiNrFkX9bPT2T3H+s0bo37y2Imor8VTWb/KdmzKrk0HJ7NrU1VVZzl7zx7rG8ry8NqxNDsV9attJbf/PkkAAAAAVJWRAAAAAOgyEgAAAABVZSQAAAAAuowEAAAAQFUZCQAAAIAuIwEAAABQVUYCAAAAoMtIAAAAAFSVkQAAAADoMhIAAAAAVVXV6HQ6naf7JAAAAICnn08SAAAAAFVlJAAAAAC6jAQAAABAVRkJAAAAgC4jAQAAAFBVRgIAAACgy0gAAAAAVJWRAAAAAOgyEgAAAABVVfX/A/qJ63Y74Lk7AAAAAElFTkSuQmCC"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"from PIL import Image\n\nclass dataset(Dataset): #dataset class\n    def __init__(self, li, transform=None):\n        super().__init__()\n        self.li = li\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.li)\n    \n    def __getitem__(self, index):\n        image_path, label = self.li[index]\n        img = cv2.imread(image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n        img = Image.fromarray(img)  \n        \n        if self.transform is not None:\n            img = self.transform(img)\n        \n        rich_img, poor_img = smash_reconstruction(img, patch_size=16) \n        rich = torch.from_numpy(concat_high_pass(high_pass(rich_img)))\n        poor = torch.from_numpy(concat_high_pass(high_pass(poor_img)))\n        \n        return {\n            'rich': rich.permute(2, 0, 1),\n            'poor': poor.permute(2, 0, 1),\n            'label': label\n        }","metadata":{"_uuid":"e29941c4-312b-4f1f-8500-44ac8cb5a809","_cell_guid":"1c51a03f-66d0-459d-a874-9896075ba63c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-04T21:30:38.176681Z","iopub.execute_input":"2024-12-04T21:30:38.177036Z","iopub.status.idle":"2024-12-04T21:30:38.184005Z","shell.execute_reply.started":"2024-12-04T21:30:38.177005Z","shell.execute_reply":"2024-12-04T21:30:38.182987Z"},"trusted":true},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":"# **Augmentations**","metadata":{}},{"cell_type":"code","source":"# Augmentations as mentioned in CNNSpot\nimport torch\nimport torchvision.transforms as T\nimport PIL\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport io\nimport random\nfrom typing import Optional\n\nclass JPEGCompression:\n    def __init__(self, quality_range=(30, 100), p=0.5, use_pil_jpeg=True):\n        self.quality_range = quality_range\n        self.p = p\n        self.use_pil_jpeg = use_pil_jpeg\n    \n    def __call__(self, img):\n        if random.random() > self.p:\n            return img\n            \n        quality = random.randint(self.quality_range[0], self.quality_range[1])\n        \n        if self.use_pil_jpeg:\n\n            buffer = io.BytesIO()\n            img.save(buffer, format='JPEG', quality=quality)\n            buffer.seek(0)\n            img = Image.open(buffer)\n        else:\n\n            img_np = np.array(img)\n            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n            _, encoded_img = cv2.imencode('.jpg', cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR), encode_param)\n            decoded_img = cv2.imdecode(encoded_img, cv2.IMREAD_COLOR)\n            img = Image.fromarray(cv2.cvtColor(decoded_img, cv2.COLOR_BGR2RGB))\n            \n        return img\n\nclass RandomGaussianBlur:\n    def __init__(self, sigma_range=(0, 3), p=0.5):\n        self.sigma_range = sigma_range\n        self.p = p\n    \n    def __call__(self, img):\n        if random.random() > self.p:\n            return img\n            \n        sigma = random.uniform(self.sigma_range[0], self.sigma_range[1])\n        return img.filter(PIL.ImageFilter.GaussianBlur(radius=sigma))\n\n\ndef get_transforms(augmentation_type: str = \"blur+jpeg_0.5\") -> tuple:\n    \"\"\"\n    Get training and testing transforms based on specified augmentation type for 32x32 images.\n    \n    Args:\n        augmentation_type: One of \"no_aug\", \"gaussian_blur\", \"jpeg\", \"blur+jpeg_0.5\", \"blur+jpeg_0.1\"\n    \n    Returns:\n        tuple: (train_transform, test_transform)\n    \"\"\"\n    # Base transforms that are always applied\n    base_transform = [\n        T.RandomHorizontalFlip(p=0.2),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]\n    \n    # Test transform (no augmentations)\n    test_transform = T.Compose([\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Add augmentations based on type\n    if augmentation_type == \"no_aug\":\n        train_transform = base_transform\n    \n    elif augmentation_type == \"gaussian_blur\":\n        train_transform = [RandomGaussianBlur(p=0.5)] + base_transform\n    \n    elif augmentation_type == \"jpeg\":\n        train_transform = [\n            JPEGCompression(p=0.5, use_pil_jpeg=random.choice([True, False]))\n        ] + base_transform\n    \n    elif augmentation_type == \"blur+jpeg_0.5\":\n        train_transform = [\n            RandomGaussianBlur(p=0.5),\n            JPEGCompression(p=0.5, use_pil_jpeg=random.choice([True, False]))\n        ] + base_transform\n    \n    elif augmentation_type == \"blur+jpeg_0.2\":\n        train_transform = [\n            RandomGaussianBlur(p=0.2),\n            JPEGCompression(p=0.2, use_pil_jpeg=random.choice([True, False]))\n        ] + base_transform\n    \n    else:\n        raise ValueError(f\"Unknown augmentation type: {augmentation_type}\")\n    \n    return T.Compose(train_transform), test_transform\n\ndef load_and_transform_image(image_path: str, transform: T.Compose) -> torch.Tensor:\n    \"\"\"\n    Load and transform an image from a path.\n    \n    Args:\n        image_path: Path to the image\n        transform: Transformation to apply\n    \n    Returns:\n        torch.Tensor: Transformed image\n    \"\"\"\n    img = Image.open(image_path).convert('RGB')\n    return transform(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:30:41.121156Z","iopub.execute_input":"2024-12-04T21:30:41.122002Z","iopub.status.idle":"2024-12-04T21:30:41.137637Z","shell.execute_reply.started":"2024-12-04T21:30:41.121944Z","shell.execute_reply":"2024-12-04T21:30:41.136776Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"train_transform, test_transform = get_transforms(augmentation_type=\"blur+jpeg_0.2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:30:41.496334Z","iopub.execute_input":"2024-12-04T21:30:41.496627Z","iopub.status.idle":"2024-12-04T21:30:41.500749Z","shell.execute_reply.started":"2024-12-04T21:30:41.496599Z","shell.execute_reply":"2024-12-04T21:30:41.499897Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"# **Dataset and Dataloader**","metadata":{}},{"cell_type":"code","source":"train_data = dataset(train_li)\ntest_data = dataset(test_li)","metadata":{"_uuid":"964aba8b-7c6c-45fc-86d4-ae6a5f49d292","_cell_guid":"e20658d8-91e5-4ca9-b935-8239701bcad4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-04T21:30:42.100811Z","iopub.execute_input":"2024-12-04T21:30:42.101437Z","iopub.status.idle":"2024-12-04T21:30:42.105012Z","shell.execute_reply.started":"2024-12-04T21:30:42.101405Z","shell.execute_reply":"2024-12-04T21:30:42.104169Z"},"trusted":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":"train_load = DataLoader(train_data, shuffle=True, batch_size=32)\ntest_load = DataLoader(test_data, shuffle=False, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T21:30:42.820334Z","iopub.execute_input":"2024-12-04T21:30:42.820677Z","iopub.status.idle":"2024-12-04T21:30:42.825318Z","shell.execute_reply.started":"2024-12-04T21:30:42.820648Z","shell.execute_reply":"2024-12-04T21:30:42.824359Z"},"trusted":true},"outputs":[],"execution_count":58},{"cell_type":"code","source":"iterator = iter(train_load)\na, b, c = next(iterator).values()\na.shape","metadata":{"execution":{"iopub.status.busy":"2024-12-04T21:30:44.468507Z","iopub.execute_input":"2024-12-04T21:30:44.469132Z","iopub.status.idle":"2024-12-04T21:30:44.771094Z","shell.execute_reply.started":"2024-12-04T21:30:44.469099Z","shell.execute_reply":"2024-12-04T21:30:44.770224Z"},"trusted":true},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 90, 32, 16])"},"metadata":{}}],"execution_count":59},{"cell_type":"markdown","source":"# **Model Architecture**","metadata":{}},{"cell_type":"code","source":"class Residual(nn.Module): # Model to generate residuals of an image\n    def __init__(self):\n        super().__init__()\n        self.poor_cnn = nn.Sequential(\n            nn.Conv2d(90, 64, 3, 1, 1),\n            nn.BatchNorm2d(64),\n            nn.Hardtanh()\n        )\n        self.rich_cnn = nn.Sequential(\n            nn.Conv2d(90, 64, 3, 1, 1),\n            nn.BatchNorm2d(64),\n            nn.Hardtanh()\n        )\n    \n    def forward(self, rich, poor):\n        rich = rich.float()\n        poor = poor.float()\n        rich = self.rich_cnn(rich)\n        poor = self.poor_cnn(poor)\n        residual = rich - poor\n        return residual","metadata":{"execution":{"iopub.status.busy":"2024-12-04T21:30:47.053115Z","iopub.execute_input":"2024-12-04T21:30:47.053872Z","iopub.status.idle":"2024-12-04T21:30:47.059278Z","shell.execute_reply.started":"2024-12-04T21:30:47.053839Z","shell.execute_reply":"2024-12-04T21:30:47.058460Z"},"trusted":true},"outputs":[],"execution_count":60},{"cell_type":"code","source":"class Classifier(nn.Module): #Classification model based on the residuals of an image\n    def __init__(self):\n        super().__init__()\n        \n        self.features = nn.Sequential(\n            # Convolutional Block 1\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),  \n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            \n            nn.AvgPool2d(kernel_size=2),\n\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            \n            nn.AvgPool2d(kernel_size=2),\n\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            \n            nn.AvgPool2d(kernel_size=2),\n\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            \n            nn.MaxPool2d(kernel_size=2)\n        )\n        \n        self.fc = nn.Linear(64, 2)\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-12-04T21:30:47.590454Z","iopub.execute_input":"2024-12-04T21:30:47.591160Z","iopub.status.idle":"2024-12-04T21:30:47.601131Z","shell.execute_reply.started":"2024-12-04T21:30:47.591126Z","shell.execute_reply":"2024-12-04T21:30:47.600240Z"},"trusted":true},"outputs":[],"execution_count":61},{"cell_type":"code","source":"n_epochs = 10\nresidual = Residual().to(device)\nclassifier = Classifier().to(device)\n\n# Combine the parameters of both models for the optimizer\nparams = list(residual.parameters()) + list(classifier.parameters())\noptimizer = torch.optim.Adam(params, weight_decay=0.01)\nloss_fn = nn.CrossEntropyLoss()\naccuracy_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to(device)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=1e-3, steps_per_epoch=len(train_load), epochs=n_epochs\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T21:30:48.339738Z","iopub.execute_input":"2024-12-04T21:30:48.340362Z","iopub.status.idle":"2024-12-04T21:30:48.356703Z","shell.execute_reply.started":"2024-12-04T21:30:48.340328Z","shell.execute_reply":"2024-12-04T21:30:48.355719Z"},"trusted":true},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"# **Training Loop**","metadata":{}},{"cell_type":"code","source":"best_val_accuracy = 0.0  # Initialize the best validation accuracy\n\nfor epoch in range(n_epochs):\n    residual.train()\n    classifier.train()\n    train_loss = 0.0\n    accuracy_metric.reset()  \n\n    for batch in tqdm(train_load):\n\n        rich = batch['rich'].to(device)\n        poor = batch['poor'].to(device)\n        labels = batch['label'].to(device)\n\n        # Forward pass through the Residual and Classifier\n        residual_output = residual(rich, poor)\n        predictions = classifier(residual_output)\n\n        loss = loss_fn(predictions, labels)\n        train_loss += loss.item() / len(train_load)\n\n        accuracy_metric.update(predictions, labels)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    # Compute overall training accuracy for this epoch\n    train_accuracy = accuracy_metric.compute().item()\n\n    # Update learning rate\n    scheduler.step()\n\n    # Print training loss and accuracy\n    print(f\"Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n\n    # Evaluation phase\n    residual.eval()\n    classifier.eval()\n    accuracy_metric.reset()  # Reset metric for validation accuracy\n\n    with torch.no_grad():\n        for batch in tqdm(test_load):\n            rich = batch['rich'].to(device)\n            poor = batch['poor'].to(device)\n            labels = batch['label'].to(device)\n\n            # Forward pass\n            residual_output = residual(rich, poor)\n            predictions = classifier(residual_output)\n\n            # Update validation accuracy\n            accuracy_metric.update(predictions, labels)\n\n    # Compute validation accuracy\n    val_accuracy = accuracy_metric.compute().item()\n    print(f\"Epoch {epoch+1}, Validation Accuracy: {val_accuracy:.4f}\")\n\n    # Save model with epoch number and validation accuracy in the filename\n    model_filename = f\"model_epoch_{epoch+1}_val_{val_accuracy:.4f}.pth\"\n    torch.save({\n        'residual_state_dict': residual.state_dict(),\n        'classifier_state_dict': classifier.state_dict(),\n    }, model_filename)\n    print(f\"Model saved as {model_filename}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}