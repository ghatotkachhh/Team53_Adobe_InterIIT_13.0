{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726},{"sourceId":9990766,"sourceType":"datasetVersion","datasetId":6148680},{"sourceId":9994702,"sourceType":"datasetVersion","datasetId":6148209},{"sourceId":10022839,"sourceType":"datasetVersion","datasetId":6172005},{"sourceId":10107530,"sourceType":"datasetVersion","datasetId":6235078},{"sourceId":10111327,"sourceType":"datasetVersion","datasetId":6237996},{"sourceId":10113675,"sourceType":"datasetVersion","datasetId":6239791},{"sourceId":10105169,"sourceType":"datasetVersion","datasetId":6233218}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import all necessary libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchmetrics\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport os\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import *\n# from tqdm.notebook import tqdm_notebook as tqdm\nfrom transformers import CLIPModel, CLIPProcessor\nfrom torchvision import transforms\nimport random\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\nimport torchvision.models as models\nfrom safetensors.torch import load_file\nimport json\nimport random\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:25:44.529128Z","iopub.execute_input":"2024-12-06T08:25:44.529663Z","iopub.status.idle":"2024-12-06T08:26:06.889226Z","shell.execute_reply.started":"2024-12-06T08:25:44.529599Z","shell.execute_reply":"2024-12-06T08:26:06.888044Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# **Import required models**","metadata":{}},{"cell_type":"markdown","source":"# ***1. Densenet***","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, model, metric_name=\"loss\", mode=\"min\"):\n        self.model = model\n        self.metric_name = metric_name\n        self.mode = mode\n        self.counter = 0\n        self.best_metric_value = float(\"inf\") if mode == \"min\" else 0.0\n\n    def __call__(self, metrics, last_epoch=False):\n        metric_value = metrics[self.metric_name]\n        delta = metric_value - self.best_metric_value\n        improvement = delta > 0 if self.mode == \"max\" else delta < 0\n\n        if improvement:\n            self.counter = 0\n            self.best_metric_value = metric_value\n\n            self.model.save(\"_checkpoint\")\n        else:\n            self.counter += 1\n\n        should_stop = self.counter >= PATIENCE\n\n        if should_stop or last_epoch:\n            self.model.load(\"_checkpoint\")\n\n        return should_stop\n\n\nclass CnnBase(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.initialize()\n\n        self.optimizer = torch.optim.Adam(self.parameters())\n        self.loss_function = nn.BCELoss()\n\n        self.to(device)\n\n    def file_name(self, suffix=\"\"):\n        return f\"{OUTPUT_DIR}/{self.name}{suffix}.pt\"\n\n    def get_metrics(self, split, desc_prefix=\"\"):\n        with torch.no_grad() if split == \"test\" else nullcontext():\n            if split == \"train\":\n                self.train()\n            elif split == \"test\":\n                self.eval()\n\n            desc = {split: f\"{desc_prefix}{split.title()}ing\" for split in DATA_SPLITS}\n\n            total_loss, total_items = 0.0, 0\n            tp, tn, fp, fn = 0, 0, 0, 0\n\n            for images, labels in (\n                tqdm(\n                    data_loaders[split],\n                    desc=desc[split],\n                    file=sys.stdout,\n                )\n                if VERBOSE\n                else data_loaders[split]\n            ):\n                images = images.to(device)\n                labels = labels.to(device).unsqueeze(1)\n                items = images.shape[0]\n\n                if split == \"train\":\n                    self.optimizer.zero_grad()\n\n                outputs = self(images)\n                predictions = (outputs >= 0.5).float()\n                loss = self.loss_function(outputs, labels.float())\n\n                if split == \"train\":\n                    loss.backward()\n                    self.optimizer.step()\n\n                total_loss += loss.item() * items\n                total_items += items\n\n                tp += ((predictions == 1) & (labels == 1)).sum().item()\n                tn += ((predictions == 0) & (labels == 0)).sum().item()\n                fp += ((predictions == 1) & (labels == 0)).sum().item()\n                fn += ((predictions == 0) & (labels == 1)).sum().item()\n\n            loss = total_loss / total_items\n            accuracy = (tp + tn) / (tp + tn + fp + fn)\n            precision = tp / (tp + fp)\n            recall = tp / (tp + fn)\n            f1_score = 2 * (precision * recall) / (precision + recall)\n\n            confusion_matrix = torch.tensor(\n                [\n                    [tn, fp],\n                    [fn, tp],\n                ]\n            ).numpy()\n\n            return {\n                \"loss\": loss,\n                \"accuracy\": accuracy,\n                \"precision\": precision,\n                \"recall\": recall,\n                \"f1_score\": f1_score,\n                \"confusion_matrix\": confusion_matrix,\n            }\n\n    def fit(self):\n        early_stopping = EarlyStopping(self)\n        history = {}\n\n        for epoch in range(EPOCHS):\n            print(f\"Epoch {epoch + 1}/{EPOCHS}{':' if VERBOSE else '...'}\")\n\n            metrics = {\n                split: self.get_metrics(split, desc_prefix=\"  \")\n                for split in DATA_SPLITS\n            }\n\n            self.update_history(history, metrics)\n\n            if VERBOSE:\n                self.print_results(metrics)\n\n            if early_stopping(metrics[\"test\"], last_epoch=epoch == (EPOCHS - 1)):\n                break\n\n        return self.get_metrics(split=\"test\"), history\n\n    def save(self, suffix=\"\"):\n        torch.save(\n            {\n                \"model\": self.state_dict(),\n                \"optimizer\": self.optimizer.state_dict(),\n            },\n            self.file_name(suffix),\n        )\n\n    def load(self, suffix=\"\"):\n        checkpoint = torch.load(self.file_name(suffix))\n\n        self.load_state_dict(checkpoint[\"model\"])\n        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    def plot_am(self):\n        print(\"\\nAblationCAM:\")\n\n        tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n\n        am = AblationCAM(model=self, target_layers=self.am_target_layers)\n\n        def image_mapper(image):\n            grayscale_am = am(\n                input_tensor=image.unsqueeze(0),\n                targets=[ClassifierOutputTarget(0)],\n            )[0]\n\n            return show_cam_on_image(\n                image.permute(1, 2, 0).numpy(),\n                grayscale_am,\n                use_rgb=True,\n            )\n\n        show_batches(\"test\", image_mapper, show_label=False)\n\n        tqdm.__init__ = partialmethod(tqdm.__init__, disable=False)\n\n    def run_routine(self):\n        print(f\"Total Parameters: {self.total_params}\\n\")\n        \n        final_metrics, history = self.fit()\n\n        self.save()\n\n        print(\"\\nFinal Metrics:\")\n        pp(final_metrics)\n\n        print(\"\\nConfusion Matrix:\")\n        \n        with plt.style.context([\"science\", \"ieee\", \"no-latex\"]):\n            cm_display = ConfusionMatrixDisplay(\n                final_metrics[\"confusion_matrix\"],\n                display_labels=LABELS,\n            )\n\n            cm_display.plot(cmap=plt.cm.Blues)\n\n            self.plot_history(history)\n\n        self.plot_am()\n\n    def plot_history(self, history):\n        history = pd.DataFrame(history)\n\n        for i, metric in enumerate(\n            [\"loss\", \"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n        ):\n            columns = [f\"{split}_{metric}\" for split in DATA_SPLITS]\n\n            plt.figure()\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(metric.title())\n\n            for column in columns:\n                plt.plot(history.index, history[column], label=column.split(\"_\")[0].title())\n\n            plt.legend(loc=\"best\")\n            plt.show()\n\n    @staticmethod\n    def update_history(history, metrics):\n        for split in DATA_SPLITS:\n            for key, value in metrics[split].items():\n                if not isinstance(value, float):\n                    continue\n\n                key = f\"{split}_{key}\"\n\n                if key not in history:\n                    history[key] = []\n\n                history[key].append(value)\n\n    @staticmethod\n    def print_results(metrics):\n        results = f\"  Results:\"\n\n        for split in DATA_SPLITS:\n            results += f\"\\n    {split.title()}:\"\n\n            for key, value in metrics[split].items():\n                if isinstance(value, float):\n                    results += f\"\\n      {key}: {value:.4f}\"\n\n        print(results)\n        \n    @property\n    def total_params(self):\n        return sum(p.numel() for p in self.parameters())\n\n\n\nclass CifakeNet(CnnBase):\n    def initialize(self):\n        self.name = \"cifakenet\"\n        densenet = models.densenet121(pretrained=True)\n        self.features = densenet.features\n        self.mlp_head = nn.Sequential(\n            nn.Linear(densenet.classifier.in_features, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Sigmoid(),\n        )\n        self.am_target_layers = [self.features]\n\n    def forward(self, x):\n        \n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.mlp_head(x)\n\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:06.891190Z","iopub.execute_input":"2024-12-06T08:26:06.891826Z","iopub.status.idle":"2024-12-06T08:26:06.925332Z","shell.execute_reply.started":"2024-12-06T08:26:06.891779Z","shell.execute_reply":"2024-12-06T08:26:06.924100Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dense_model = CifakeNet()\ndense_model.load_state_dict(torch.load(\"/kaggle/input/densenet-without-v2/_checkpoint (2).pth\", map_location = torch.device('cpu')))\ndense_model = dense_model.to(device)\nprint(f\"The parameters of dense_model are on: {next(dense_model.parameters()).device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:06.926784Z","iopub.execute_input":"2024-12-06T08:26:06.927241Z","iopub.status.idle":"2024-12-06T08:26:08.006976Z","shell.execute_reply.started":"2024-12-06T08:26:06.927193Z","shell.execute_reply":"2024-12-06T08:26:08.005868Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n100%|██████████| 30.8M/30.8M [00:00<00:00, 134MB/s] \n","output_type":"stream"},{"name":"stdout","text":"The parameters of dense_model are on: cpu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# ***2. ViT***","metadata":{}},{"cell_type":"code","source":"from transformers import ViTImageProcessor\nfrom transformers import ViTForImageClassification\nmodel_str = 'WinKawaks/vit-tiny-patch16-224'\nprocessor = ViTImageProcessor.from_pretrained(model_str)\n\nvit_model = ViTForImageClassification.from_pretrained(model_str, num_labels=2,ignore_mismatched_sizes=True)\n\nvit_model.load_state_dict(torch.load(\"/kaggle/input/vit-tiny/vit tiny.pt\", map_location = torch.device('cpu')))\nvit_model = vit_model.to(device)\nprint(f\"The parameters of vit_model are on: {next(vit_model.parameters()).device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:08.009070Z","iopub.execute_input":"2024-12-06T08:26:08.009402Z","iopub.status.idle":"2024-12-06T08:26:09.331508Z","shell.execute_reply.started":"2024-12-06T08:26:08.009369Z","shell.execute_reply":"2024-12-06T08:26:09.330158Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47e04345b4c14a23a823d92881147452"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b9c1979334b4ae49388e087836cb5a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/22.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84ad5efe2fde4a518c7fd6eb468c8603"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([2, 192]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"The parameters of vit_model are on: cpu\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **Define dataset classes**","metadata":{}},{"cell_type":"code","source":"class dataset(Dataset):\n    def __init__(self, li, transform, train):\n        super().__init__()\n        self.li = li\n        self.transform = transform\n        self.train = train\n\n    def __len__(self):\n        return len(self.li)\n\n    def __getitem__(self, idx):\n        img = self.li[idx][0]\n        # img_arr = np.array(self.li[idx][0])\n        y = self.li[idx][1]\n        if self.train:\n            try:\n                img = self.transform(img)\n            except:\n                img = self.transform(np.array(img))\n        return img, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:09.332851Z","iopub.execute_input":"2024-12-06T08:26:09.333183Z","iopub.status.idle":"2024-12-06T08:26:09.340195Z","shell.execute_reply.started":"2024-12-06T08:26:09.333147Z","shell.execute_reply":"2024-12-06T08:26:09.338957Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# **Tranformations**","metadata":{}},{"cell_type":"code","source":"#transformation for densenet\ntransform_dense = transforms.Compose([\n    transforms.Resize(32),\n    transforms.ToTensor()\n])\n\n#Transformation for ViT\nimage_mean, image_std = processor.image_mean, processor.image_std\nsize = processor.size[\"height\"]\nprint(f\"Size of the input image that ViT processor takes: {size}x{size}\")\n\ntransform_vit = transforms.Compose([\n    transforms.Resize((size, size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=image_mean, std=image_std)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:09.341872Z","iopub.execute_input":"2024-12-06T08:26:09.342424Z","iopub.status.idle":"2024-12-06T08:26:09.355087Z","shell.execute_reply.started":"2024-12-06T08:26:09.342376Z","shell.execute_reply":"2024-12-06T08:26:09.354144Z"}},"outputs":[{"name":"stdout","text":"Size of the input image that ViT processor takes: 224x224\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# **Write all necessary function definitions**","metadata":{}},{"cell_type":"code","source":"#This is the function to extract images and their respective indices in the format given in the adobe submission guideline.\ndef extract_test_data(path, l, transform):\n    li = []\n    for img_path in tqdm(l):\n        idx = int(img_path.split(\".\")[0])\n        img = Image.open(os.path.join(path, img_path))\n        img = img.convert(\"RGB\")\n        li.append([img, idx])\n\n    data = dataset(li, transform, True)\n    return DataLoader(data, batch_size=32, shuffle=False, num_workers=os.cpu_count()), li","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:09.356285Z","iopub.execute_input":"2024-12-06T08:26:09.356655Z","iopub.status.idle":"2024-12-06T08:26:09.368434Z","shell.execute_reply.started":"2024-12-06T08:26:09.356614Z","shell.execute_reply":"2024-12-06T08:26:09.367322Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def dense_model_infer(dataload, prob=False):\n    label_dense = []\n    pred_dense = []\n    dense_model.eval()\n    with torch.inference_mode():\n        for img, y in tqdm(dataload):\n            label_dense.append(y)\n            img=img.to(device)\n            if prob:\n                y_pred = (dense_model(img)).cpu()\n            else:\n                y_pred = (torch.round(dense_model(img))).cpu()\n            pred_dense.append(y_pred)\n    return pred_dense, label_dense","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:09.370040Z","iopub.execute_input":"2024-12-06T08:26:09.370356Z","iopub.status.idle":"2024-12-06T08:26:09.386226Z","shell.execute_reply.started":"2024-12-06T08:26:09.370326Z","shell.execute_reply":"2024-12-06T08:26:09.385068Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def vit_model_infer(dataload, prob=False):\n    label_vit = []\n    pred_vit = []\n    with torch.inference_mode():\n        for img, y in tqdm(dataload):\n            label_vit.append(y)\n            img=img.to(device)\n            if prob:\n                y_pred = vit_model(img)[0][:,0].cpu().detach().numpy()\n            else:\n                y_pred = np.argmin(vit_model(img)[0].cpu().detach().numpy(), axis=1)\n            pred_vit.append(y_pred)\n    return pred_vit, label_vit ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:09.387517Z","iopub.execute_input":"2024-12-06T08:26:09.387968Z","iopub.status.idle":"2024-12-06T08:26:09.397219Z","shell.execute_reply.started":"2024-12-06T08:26:09.387924Z","shell.execute_reply":"2024-12-06T08:26:09.396173Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def weighted_soft_ensemble(predictions, weights):\n    \"\"\"\n    Compute the weighted soft ensemble for binary classification.\n    \n    Args:\n        predictions (list of array-like): A list where each element is a numpy array or a list\n                                          of shape (n_samples,), representing\n                                          the predicted probabilities for the positive class.\n        weights (list of float): A list of weights corresponding to the models.\n                                 The length of this list must match the length of `predictions`.\n    \n    Returns:\n        numpy.ndarray: An array of shape (n_samples,) representing the ensembled probabilities.\n    \"\"\"\n    # Ensure weights and predictions have the same length\n    if len(predictions) != len(weights):\n        raise ValueError(\"The number of prediction arrays must match the number of weights.\")\n    \n    # Convert predictions to numpy arrays if they are not already\n    predictions = [np.array(pred) for pred in predictions]\n    \n    # Normalize weights to sum to 1\n    normalized_weights = np.array(weights) / np.sum(weights)\n    \n    # Compute the weighted sum of prediction probabilities\n    ensembled_probs = np.zeros_like(predictions[0], dtype=float)\n    for pred, weight in zip(predictions, normalized_weights):\n        ensembled_probs += weight * pred\n    \n    return ensembled_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:09.400723Z","iopub.execute_input":"2024-12-06T08:26:09.401093Z","iopub.status.idle":"2024-12-06T08:26:09.410504Z","shell.execute_reply.started":"2024-12-06T08:26:09.401060Z","shell.execute_reply":"2024-12-06T08:26:09.409445Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from collections import Counter\nfrom typing import List\n\ndef hard_ensemble(predictions: List[List[int]]) -> List[int]:\n    \"\"\"\n    Combine predictions from multiple models using hard voting.\n\n    Args:\n        predictions (List[List[int]]): A list of lists where each inner list contains predictions\n                                       from one model. All inner lists must have the same length.\n\n    Returns:\n        List[int]: A single list of final predictions after applying hard voting.\n    \"\"\"\n    if not predictions:\n        raise ValueError(\"The predictions list cannot be empty.\")\n\n    # Check that all models provide predictions of the same length\n    prediction_length = len(predictions[0])\n    if not all(len(pred) == prediction_length for pred in predictions):\n        raise ValueError(\"All prediction lists must have the same length.\")\n\n    # Transpose the predictions to group predictions for each instance\n    transposed_predictions = zip(*predictions)\n\n    # Perform majority voting for each instance\n    ensemble_result = []\n    for instance_predictions in transposed_predictions:\n        # Count the frequency of each prediction\n        vote_counts = Counter(instance_predictions)\n        # Choose the most common prediction\n        majority_vote = vote_counts.most_common(1)[0][0]\n        ensemble_result.append(majority_vote)\n\n    return ensemble_result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:09.411739Z","iopub.execute_input":"2024-12-06T08:26:09.412084Z","iopub.status.idle":"2024-12-06T08:26:09.428011Z","shell.execute_reply.started":"2024-12-06T08:26:09.412051Z","shell.execute_reply":"2024-12-06T08:26:09.426681Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def flatten(li, inv=False):\n    if inv:\n        return [1-j for i in li for j in i]\n    else:\n        return [j for i in li for j in i]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:09.429198Z","iopub.execute_input":"2024-12-06T08:26:09.429565Z","iopub.status.idle":"2024-12-06T08:26:09.442823Z","shell.execute_reply.started":"2024-12-06T08:26:09.429530Z","shell.execute_reply":"2024-12-06T08:26:09.441675Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def return_dict(pred, idx):\n    li = [0 for i in range(len(pred))]\n    for i, j in enumerate(pred):\n        if j==1:\n            li[i] = {\n                \"index\": idx[i],\n                \"prediction\": \"fake\"\n            }\n        if j==0:\n            li[i] = {\n                \"index\": idx[i],\n                \"prediction\": \"real\"\n            }\n    return li","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:09.444652Z","iopub.execute_input":"2024-12-06T08:26:09.445109Z","iopub.status.idle":"2024-12-06T08:26:09.454794Z","shell.execute_reply.started":"2024-12-06T08:26:09.445075Z","shell.execute_reply":"2024-12-06T08:26:09.453566Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def or_gate(li):\n    m = len(li[0])\n    or_li = [0 for i in range(m)]\n    for i in range(m):\n        sum=0\n        for l in li:\n            sum += l[i]\n        if sum>=1:\n            or_li[i] = 1\n    return or_li","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:09.456015Z","iopub.execute_input":"2024-12-06T08:26:09.456327Z","iopub.status.idle":"2024-12-06T08:26:09.466204Z","shell.execute_reply.started":"2024-12-06T08:26:09.456298Z","shell.execute_reply":"2024-12-06T08:26:09.465191Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# **Convert Images to JPEG format**","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\n\n# Source directory containing the PNG images\nsource_dir = \"/kaggle/input/interiit-test/perturbed_images_32\"\n# Destination directory to save the converted JPG images\noutput_dir = \"/kaggle/working/perturbed_images_32_jpg\"\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\ndef convert_png_to_jpg(source_folder, output_folder):\n    converted_count = 0  # Counter for converted images\n    for root, dirs, files in os.walk(source_folder):\n        # Recreate the folder structure in the output directory\n        relative_path = os.path.relpath(root, source_folder)\n        target_folder = os.path.join(output_folder, relative_path)\n        os.makedirs(target_folder, exist_ok=True)\n\n        # Convert PNG to JPG\n        for file in files:\n            if file.endswith(\".png\"):\n                source_file = os.path.join(root, file)\n                target_file = os.path.join(target_folder, file.replace(\".png\", \".jpg\"))\n\n                try:\n                    # Open the image\n                    with Image.open(source_file) as img:\n                        # Convert to RGB (required for JPG format)\n                        rgb_img = img.convert(\"RGB\")\n                        # Save the image as JPG\n                        rgb_img.save(target_file, \"JPEG\")\n                    print(f\"Converted: {source_file} -> {target_file}\")\n                    converted_count += 1  # Increment the counter\n                except Exception as e:\n                    print(f\"Error converting {source_file}: {e}\")\n\n    return converted_count\n\n# Call the function to convert all PNG images to JPG\ntotal_converted = convert_png_to_jpg(source_dir, output_dir)\n\n# Print the total count of converted images\nprint(f\"Total number of PNG images converted to JPG: {total_converted}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:26:26.597125Z","iopub.execute_input":"2024-12-06T08:26:26.597552Z","iopub.status.idle":"2024-12-06T08:26:28.481779Z","shell.execute_reply.started":"2024-12-06T08:26:26.597513Z","shell.execute_reply":"2024-12-06T08:26:28.480670Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Converted: /kaggle/input/interiit-test/perturbed_images_32/173.png -> /kaggle/working/perturbed_images_32_jpg/./173.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/248.png -> /kaggle/working/perturbed_images_32_jpg/./248.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/94.png -> /kaggle/working/perturbed_images_32_jpg/./94.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/236.png -> /kaggle/working/perturbed_images_32_jpg/./236.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/48.png -> /kaggle/working/perturbed_images_32_jpg/./48.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/227.png -> /kaggle/working/perturbed_images_32_jpg/./227.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/238.png -> /kaggle/working/perturbed_images_32_jpg/./238.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/61.png -> /kaggle/working/perturbed_images_32_jpg/./61.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/222.png -> /kaggle/working/perturbed_images_32_jpg/./222.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/278.png -> /kaggle/working/perturbed_images_32_jpg/./278.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/37.png -> /kaggle/working/perturbed_images_32_jpg/./37.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/231.png -> /kaggle/working/perturbed_images_32_jpg/./231.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/148.png -> /kaggle/working/perturbed_images_32_jpg/./148.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/35.png -> /kaggle/working/perturbed_images_32_jpg/./35.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/232.png -> /kaggle/working/perturbed_images_32_jpg/./232.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/177.png -> /kaggle/working/perturbed_images_32_jpg/./177.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/142.png -> /kaggle/working/perturbed_images_32_jpg/./142.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/104.png -> /kaggle/working/perturbed_images_32_jpg/./104.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/133.png -> /kaggle/working/perturbed_images_32_jpg/./133.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/70.png -> /kaggle/working/perturbed_images_32_jpg/./70.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/162.png -> /kaggle/working/perturbed_images_32_jpg/./162.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/186.png -> /kaggle/working/perturbed_images_32_jpg/./186.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/73.png -> /kaggle/working/perturbed_images_32_jpg/./73.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/92.png -> /kaggle/working/perturbed_images_32_jpg/./92.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/261.png -> /kaggle/working/perturbed_images_32_jpg/./261.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/153.png -> /kaggle/working/perturbed_images_32_jpg/./153.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/167.png -> /kaggle/working/perturbed_images_32_jpg/./167.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/204.png -> /kaggle/working/perturbed_images_32_jpg/./204.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/75.png -> /kaggle/working/perturbed_images_32_jpg/./75.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/296.png -> /kaggle/working/perturbed_images_32_jpg/./296.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/270.png -> /kaggle/working/perturbed_images_32_jpg/./270.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/89.png -> /kaggle/working/perturbed_images_32_jpg/./89.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/164.png -> /kaggle/working/perturbed_images_32_jpg/./164.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/11.png -> /kaggle/working/perturbed_images_32_jpg/./11.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/213.png -> /kaggle/working/perturbed_images_32_jpg/./213.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/114.png -> /kaggle/working/perturbed_images_32_jpg/./114.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/185.png -> /kaggle/working/perturbed_images_32_jpg/./185.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/240.png -> /kaggle/working/perturbed_images_32_jpg/./240.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/281.png -> /kaggle/working/perturbed_images_32_jpg/./281.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/176.png -> /kaggle/working/perturbed_images_32_jpg/./176.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/95.png -> /kaggle/working/perturbed_images_32_jpg/./95.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/97.png -> /kaggle/working/perturbed_images_32_jpg/./97.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/124.png -> /kaggle/working/perturbed_images_32_jpg/./124.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/119.png -> /kaggle/working/perturbed_images_32_jpg/./119.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/256.png -> /kaggle/working/perturbed_images_32_jpg/./256.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/100.png -> /kaggle/working/perturbed_images_32_jpg/./100.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/211.png -> /kaggle/working/perturbed_images_32_jpg/./211.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/76.png -> /kaggle/working/perturbed_images_32_jpg/./76.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/193.png -> /kaggle/working/perturbed_images_32_jpg/./193.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/191.png -> /kaggle/working/perturbed_images_32_jpg/./191.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/163.png -> /kaggle/working/perturbed_images_32_jpg/./163.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/31.png -> /kaggle/working/perturbed_images_32_jpg/./31.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/138.png -> /kaggle/working/perturbed_images_32_jpg/./138.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/4.png -> /kaggle/working/perturbed_images_32_jpg/./4.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/225.png -> /kaggle/working/perturbed_images_32_jpg/./225.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/192.png -> /kaggle/working/perturbed_images_32_jpg/./192.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/43.png -> /kaggle/working/perturbed_images_32_jpg/./43.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/229.png -> /kaggle/working/perturbed_images_32_jpg/./229.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/129.png -> /kaggle/working/perturbed_images_32_jpg/./129.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/171.png -> /kaggle/working/perturbed_images_32_jpg/./171.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/287.png -> /kaggle/working/perturbed_images_32_jpg/./287.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/145.png -> /kaggle/working/perturbed_images_32_jpg/./145.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/40.png -> /kaggle/working/perturbed_images_32_jpg/./40.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/209.png -> /kaggle/working/perturbed_images_32_jpg/./209.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/197.png -> /kaggle/working/perturbed_images_32_jpg/./197.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/33.png -> /kaggle/working/perturbed_images_32_jpg/./33.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/126.png -> /kaggle/working/perturbed_images_32_jpg/./126.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/147.png -> /kaggle/working/perturbed_images_32_jpg/./147.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/85.png -> /kaggle/working/perturbed_images_32_jpg/./85.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/9.png -> /kaggle/working/perturbed_images_32_jpg/./9.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/102.png -> /kaggle/working/perturbed_images_32_jpg/./102.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/254.png -> /kaggle/working/perturbed_images_32_jpg/./254.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/245.png -> /kaggle/working/perturbed_images_32_jpg/./245.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/168.png -> /kaggle/working/perturbed_images_32_jpg/./168.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/233.png -> /kaggle/working/perturbed_images_32_jpg/./233.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/200.png -> /kaggle/working/perturbed_images_32_jpg/./200.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/255.png -> /kaggle/working/perturbed_images_32_jpg/./255.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/284.png -> /kaggle/working/perturbed_images_32_jpg/./284.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/56.png -> /kaggle/working/perturbed_images_32_jpg/./56.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/116.png -> /kaggle/working/perturbed_images_32_jpg/./116.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/299.png -> /kaggle/working/perturbed_images_32_jpg/./299.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/80.png -> /kaggle/working/perturbed_images_32_jpg/./80.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/55.png -> /kaggle/working/perturbed_images_32_jpg/./55.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/280.png -> /kaggle/working/perturbed_images_32_jpg/./280.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/130.png -> /kaggle/working/perturbed_images_32_jpg/./130.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/291.png -> /kaggle/working/perturbed_images_32_jpg/./291.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/50.png -> /kaggle/working/perturbed_images_32_jpg/./50.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/288.png -> /kaggle/working/perturbed_images_32_jpg/./288.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/218.png -> /kaggle/working/perturbed_images_32_jpg/./218.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/199.png -> /kaggle/working/perturbed_images_32_jpg/./199.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/112.png -> /kaggle/working/perturbed_images_32_jpg/./112.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/49.png -> /kaggle/working/perturbed_images_32_jpg/./49.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/268.png -> /kaggle/working/perturbed_images_32_jpg/./268.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/93.png -> /kaggle/working/perturbed_images_32_jpg/./93.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/150.png -> /kaggle/working/perturbed_images_32_jpg/./150.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/113.png -> /kaggle/working/perturbed_images_32_jpg/./113.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/59.png -> /kaggle/working/perturbed_images_32_jpg/./59.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/169.png -> /kaggle/working/perturbed_images_32_jpg/./169.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/289.png -> /kaggle/working/perturbed_images_32_jpg/./289.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/170.png -> /kaggle/working/perturbed_images_32_jpg/./170.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/194.png -> /kaggle/working/perturbed_images_32_jpg/./194.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/136.png -> /kaggle/working/perturbed_images_32_jpg/./136.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/198.png -> /kaggle/working/perturbed_images_32_jpg/./198.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/14.png -> /kaggle/working/perturbed_images_32_jpg/./14.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/65.png -> /kaggle/working/perturbed_images_32_jpg/./65.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/161.png -> /kaggle/working/perturbed_images_32_jpg/./161.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/1.png -> /kaggle/working/perturbed_images_32_jpg/./1.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/293.png -> /kaggle/working/perturbed_images_32_jpg/./293.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/108.png -> /kaggle/working/perturbed_images_32_jpg/./108.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/39.png -> /kaggle/working/perturbed_images_32_jpg/./39.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/132.png -> /kaggle/working/perturbed_images_32_jpg/./132.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/156.png -> /kaggle/working/perturbed_images_32_jpg/./156.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/107.png -> /kaggle/working/perturbed_images_32_jpg/./107.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/90.png -> /kaggle/working/perturbed_images_32_jpg/./90.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/262.png -> /kaggle/working/perturbed_images_32_jpg/./262.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/215.png -> /kaggle/working/perturbed_images_32_jpg/./215.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/20.png -> /kaggle/working/perturbed_images_32_jpg/./20.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/181.png -> /kaggle/working/perturbed_images_32_jpg/./181.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/83.png -> /kaggle/working/perturbed_images_32_jpg/./83.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/38.png -> /kaggle/working/perturbed_images_32_jpg/./38.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/151.png -> /kaggle/working/perturbed_images_32_jpg/./151.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/2.png -> /kaggle/working/perturbed_images_32_jpg/./2.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/223.png -> /kaggle/working/perturbed_images_32_jpg/./223.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/272.png -> /kaggle/working/perturbed_images_32_jpg/./272.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/10.png -> /kaggle/working/perturbed_images_32_jpg/./10.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/82.png -> /kaggle/working/perturbed_images_32_jpg/./82.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/242.png -> /kaggle/working/perturbed_images_32_jpg/./242.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/251.png -> /kaggle/working/perturbed_images_32_jpg/./251.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/174.png -> /kaggle/working/perturbed_images_32_jpg/./174.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/250.png -> /kaggle/working/perturbed_images_32_jpg/./250.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/131.png -> /kaggle/working/perturbed_images_32_jpg/./131.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/36.png -> /kaggle/working/perturbed_images_32_jpg/./36.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/157.png -> /kaggle/working/perturbed_images_32_jpg/./157.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/182.png -> /kaggle/working/perturbed_images_32_jpg/./182.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/143.png -> /kaggle/working/perturbed_images_32_jpg/./143.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/221.png -> /kaggle/working/perturbed_images_32_jpg/./221.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/208.png -> /kaggle/working/perturbed_images_32_jpg/./208.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/152.png -> /kaggle/working/perturbed_images_32_jpg/./152.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/137.png -> /kaggle/working/perturbed_images_32_jpg/./137.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/81.png -> /kaggle/working/perturbed_images_32_jpg/./81.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/239.png -> /kaggle/working/perturbed_images_32_jpg/./239.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/66.png -> /kaggle/working/perturbed_images_32_jpg/./66.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/58.png -> /kaggle/working/perturbed_images_32_jpg/./58.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/214.png -> /kaggle/working/perturbed_images_32_jpg/./214.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/122.png -> /kaggle/working/perturbed_images_32_jpg/./122.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/276.png -> /kaggle/working/perturbed_images_32_jpg/./276.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/187.png -> /kaggle/working/perturbed_images_32_jpg/./187.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/106.png -> /kaggle/working/perturbed_images_32_jpg/./106.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/279.png -> /kaggle/working/perturbed_images_32_jpg/./279.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/117.png -> /kaggle/working/perturbed_images_32_jpg/./117.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/220.png -> /kaggle/working/perturbed_images_32_jpg/./220.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/277.png -> /kaggle/working/perturbed_images_32_jpg/./277.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/300.png -> /kaggle/working/perturbed_images_32_jpg/./300.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/244.png -> /kaggle/working/perturbed_images_32_jpg/./244.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/282.png -> /kaggle/working/perturbed_images_32_jpg/./282.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/263.png -> /kaggle/working/perturbed_images_32_jpg/./263.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/166.png -> /kaggle/working/perturbed_images_32_jpg/./166.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/18.png -> /kaggle/working/perturbed_images_32_jpg/./18.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/63.png -> /kaggle/working/perturbed_images_32_jpg/./63.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/294.png -> /kaggle/working/perturbed_images_32_jpg/./294.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/264.png -> /kaggle/working/perturbed_images_32_jpg/./264.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/21.png -> /kaggle/working/perturbed_images_32_jpg/./21.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/12.png -> /kaggle/working/perturbed_images_32_jpg/./12.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/98.png -> /kaggle/working/perturbed_images_32_jpg/./98.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/127.png -> /kaggle/working/perturbed_images_32_jpg/./127.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/219.png -> /kaggle/working/perturbed_images_32_jpg/./219.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/184.png -> /kaggle/working/perturbed_images_32_jpg/./184.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/28.png -> /kaggle/working/perturbed_images_32_jpg/./28.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/190.png -> /kaggle/working/perturbed_images_32_jpg/./190.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/165.png -> /kaggle/working/perturbed_images_32_jpg/./165.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/22.png -> /kaggle/working/perturbed_images_32_jpg/./22.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/202.png -> /kaggle/working/perturbed_images_32_jpg/./202.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/247.png -> /kaggle/working/perturbed_images_32_jpg/./247.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/67.png -> /kaggle/working/perturbed_images_32_jpg/./67.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/96.png -> /kaggle/working/perturbed_images_32_jpg/./96.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/243.png -> /kaggle/working/perturbed_images_32_jpg/./243.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/34.png -> /kaggle/working/perturbed_images_32_jpg/./34.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/41.png -> /kaggle/working/perturbed_images_32_jpg/./41.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/159.png -> /kaggle/working/perturbed_images_32_jpg/./159.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/246.png -> /kaggle/working/perturbed_images_32_jpg/./246.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/195.png -> /kaggle/working/perturbed_images_32_jpg/./195.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/105.png -> /kaggle/working/perturbed_images_32_jpg/./105.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/125.png -> /kaggle/working/perturbed_images_32_jpg/./125.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/121.png -> /kaggle/working/perturbed_images_32_jpg/./121.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/216.png -> /kaggle/working/perturbed_images_32_jpg/./216.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/69.png -> /kaggle/working/perturbed_images_32_jpg/./69.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/146.png -> /kaggle/working/perturbed_images_32_jpg/./146.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/154.png -> /kaggle/working/perturbed_images_32_jpg/./154.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/267.png -> /kaggle/working/perturbed_images_32_jpg/./267.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/7.png -> /kaggle/working/perturbed_images_32_jpg/./7.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/241.png -> /kaggle/working/perturbed_images_32_jpg/./241.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/188.png -> /kaggle/working/perturbed_images_32_jpg/./188.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/86.png -> /kaggle/working/perturbed_images_32_jpg/./86.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/258.png -> /kaggle/working/perturbed_images_32_jpg/./258.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/77.png -> /kaggle/working/perturbed_images_32_jpg/./77.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/290.png -> /kaggle/working/perturbed_images_32_jpg/./290.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/29.png -> /kaggle/working/perturbed_images_32_jpg/./29.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/265.png -> /kaggle/working/perturbed_images_32_jpg/./265.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/103.png -> /kaggle/working/perturbed_images_32_jpg/./103.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/72.png -> /kaggle/working/perturbed_images_32_jpg/./72.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/144.png -> /kaggle/working/perturbed_images_32_jpg/./144.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/298.png -> /kaggle/working/perturbed_images_32_jpg/./298.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/109.png -> /kaggle/working/perturbed_images_32_jpg/./109.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/224.png -> /kaggle/working/perturbed_images_32_jpg/./224.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/71.png -> /kaggle/working/perturbed_images_32_jpg/./71.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/74.png -> /kaggle/working/perturbed_images_32_jpg/./74.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/120.png -> /kaggle/working/perturbed_images_32_jpg/./120.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/271.png -> /kaggle/working/perturbed_images_32_jpg/./271.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/118.png -> /kaggle/working/perturbed_images_32_jpg/./118.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/17.png -> /kaggle/working/perturbed_images_32_jpg/./17.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/53.png -> /kaggle/working/perturbed_images_32_jpg/./53.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/266.png -> /kaggle/working/perturbed_images_32_jpg/./266.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/196.png -> /kaggle/working/perturbed_images_32_jpg/./196.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/179.png -> /kaggle/working/perturbed_images_32_jpg/./179.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/178.png -> /kaggle/working/perturbed_images_32_jpg/./178.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/160.png -> /kaggle/working/perturbed_images_32_jpg/./160.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/140.png -> /kaggle/working/perturbed_images_32_jpg/./140.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/52.png -> /kaggle/working/perturbed_images_32_jpg/./52.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/5.png -> /kaggle/working/perturbed_images_32_jpg/./5.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/139.png -> /kaggle/working/perturbed_images_32_jpg/./139.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/3.png -> /kaggle/working/perturbed_images_32_jpg/./3.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/172.png -> /kaggle/working/perturbed_images_32_jpg/./172.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/64.png -> /kaggle/working/perturbed_images_32_jpg/./64.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/16.png -> /kaggle/working/perturbed_images_32_jpg/./16.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/203.png -> /kaggle/working/perturbed_images_32_jpg/./203.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/8.png -> /kaggle/working/perturbed_images_32_jpg/./8.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/45.png -> /kaggle/working/perturbed_images_32_jpg/./45.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/24.png -> /kaggle/working/perturbed_images_32_jpg/./24.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/32.png -> /kaggle/working/perturbed_images_32_jpg/./32.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/88.png -> /kaggle/working/perturbed_images_32_jpg/./88.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/283.png -> /kaggle/working/perturbed_images_32_jpg/./283.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/274.png -> /kaggle/working/perturbed_images_32_jpg/./274.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/115.png -> /kaggle/working/perturbed_images_32_jpg/./115.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/249.png -> /kaggle/working/perturbed_images_32_jpg/./249.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/128.png -> /kaggle/working/perturbed_images_32_jpg/./128.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/158.png -> /kaggle/working/perturbed_images_32_jpg/./158.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/6.png -> /kaggle/working/perturbed_images_32_jpg/./6.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/23.png -> /kaggle/working/perturbed_images_32_jpg/./23.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/87.png -> /kaggle/working/perturbed_images_32_jpg/./87.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/275.png -> /kaggle/working/perturbed_images_32_jpg/./275.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/51.png -> /kaggle/working/perturbed_images_32_jpg/./51.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/101.png -> /kaggle/working/perturbed_images_32_jpg/./101.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/297.png -> /kaggle/working/perturbed_images_32_jpg/./297.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/135.png -> /kaggle/working/perturbed_images_32_jpg/./135.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/123.png -> /kaggle/working/perturbed_images_32_jpg/./123.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/91.png -> /kaggle/working/perturbed_images_32_jpg/./91.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/230.png -> /kaggle/working/perturbed_images_32_jpg/./230.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/44.png -> /kaggle/working/perturbed_images_32_jpg/./44.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/189.png -> /kaggle/working/perturbed_images_32_jpg/./189.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/68.png -> /kaggle/working/perturbed_images_32_jpg/./68.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/286.png -> /kaggle/working/perturbed_images_32_jpg/./286.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/180.png -> /kaggle/working/perturbed_images_32_jpg/./180.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/46.png -> /kaggle/working/perturbed_images_32_jpg/./46.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/57.png -> /kaggle/working/perturbed_images_32_jpg/./57.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/84.png -> /kaggle/working/perturbed_images_32_jpg/./84.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/15.png -> /kaggle/working/perturbed_images_32_jpg/./15.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/210.png -> /kaggle/working/perturbed_images_32_jpg/./210.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/234.png -> /kaggle/working/perturbed_images_32_jpg/./234.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/252.png -> /kaggle/working/perturbed_images_32_jpg/./252.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/183.png -> /kaggle/working/perturbed_images_32_jpg/./183.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/30.png -> /kaggle/working/perturbed_images_32_jpg/./30.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/60.png -> /kaggle/working/perturbed_images_32_jpg/./60.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/235.png -> /kaggle/working/perturbed_images_32_jpg/./235.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/99.png -> /kaggle/working/perturbed_images_32_jpg/./99.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/155.png -> /kaggle/working/perturbed_images_32_jpg/./155.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/228.png -> /kaggle/working/perturbed_images_32_jpg/./228.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/27.png -> /kaggle/working/perturbed_images_32_jpg/./27.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/237.png -> /kaggle/working/perturbed_images_32_jpg/./237.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/292.png -> /kaggle/working/perturbed_images_32_jpg/./292.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/201.png -> /kaggle/working/perturbed_images_32_jpg/./201.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/295.png -> /kaggle/working/perturbed_images_32_jpg/./295.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/273.png -> /kaggle/working/perturbed_images_32_jpg/./273.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/269.png -> /kaggle/working/perturbed_images_32_jpg/./269.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/78.png -> /kaggle/working/perturbed_images_32_jpg/./78.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/212.png -> /kaggle/working/perturbed_images_32_jpg/./212.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/47.png -> /kaggle/working/perturbed_images_32_jpg/./47.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/207.png -> /kaggle/working/perturbed_images_32_jpg/./207.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/149.png -> /kaggle/working/perturbed_images_32_jpg/./149.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/13.png -> /kaggle/working/perturbed_images_32_jpg/./13.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/260.png -> /kaggle/working/perturbed_images_32_jpg/./260.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/253.png -> /kaggle/working/perturbed_images_32_jpg/./253.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/206.png -> /kaggle/working/perturbed_images_32_jpg/./206.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/134.png -> /kaggle/working/perturbed_images_32_jpg/./134.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/259.png -> /kaggle/working/perturbed_images_32_jpg/./259.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/25.png -> /kaggle/working/perturbed_images_32_jpg/./25.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/285.png -> /kaggle/working/perturbed_images_32_jpg/./285.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/54.png -> /kaggle/working/perturbed_images_32_jpg/./54.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/111.png -> /kaggle/working/perturbed_images_32_jpg/./111.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/62.png -> /kaggle/working/perturbed_images_32_jpg/./62.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/205.png -> /kaggle/working/perturbed_images_32_jpg/./205.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/141.png -> /kaggle/working/perturbed_images_32_jpg/./141.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/19.png -> /kaggle/working/perturbed_images_32_jpg/./19.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/110.png -> /kaggle/working/perturbed_images_32_jpg/./110.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/26.png -> /kaggle/working/perturbed_images_32_jpg/./26.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/79.png -> /kaggle/working/perturbed_images_32_jpg/./79.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/257.png -> /kaggle/working/perturbed_images_32_jpg/./257.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/42.png -> /kaggle/working/perturbed_images_32_jpg/./42.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/175.png -> /kaggle/working/perturbed_images_32_jpg/./175.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/226.png -> /kaggle/working/perturbed_images_32_jpg/./226.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/217.png -> /kaggle/working/perturbed_images_32_jpg/./217.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/.ipynb_checkpoints/perturbed_real_7_222-checkpoint.png -> /kaggle/working/perturbed_images_32_jpg/.ipynb_checkpoints/perturbed_real_7_222-checkpoint.jpg\nConverted: /kaggle/input/interiit-test/perturbed_images_32/.ipynb_checkpoints/perturbed_real_6_221-checkpoint.png -> /kaggle/working/perturbed_images_32_jpg/.ipynb_checkpoints/perturbed_real_6_221-checkpoint.jpg\nTotal number of PNG images converted to JPG: 302\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# **Infer for Densenet and ViT**","metadata":{}},{"cell_type":"code","source":"import time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:27:42.622269Z","iopub.execute_input":"2024-12-06T08:27:42.622659Z","iopub.status.idle":"2024-12-06T08:27:42.627689Z","shell.execute_reply.started":"2024-12-06T08:27:42.622627Z","shell.execute_reply":"2024-12-06T08:27:42.626616Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#Load Data\ntest_path = \"/kaggle/working/perturbed_images_32_jpg\"\ntest_li = os.listdir(test_path)\ntest_li.remove(\".ipynb_checkpoints\")\nlen(test_li)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:27:44.172778Z","iopub.execute_input":"2024-12-06T08:27:44.173899Z","iopub.status.idle":"2024-12-06T08:27:44.182196Z","shell.execute_reply.started":"2024-12-06T08:27:44.173855Z","shell.execute_reply":"2024-12-06T08:27:44.180969Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"300"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"#Create DataLoaders\ntestload_dense, test_dense_li = extract_test_data(test_path, test_li, transform_dense)\ntestload_vit, test_vit_li = extract_test_data(test_path, test_li, transform_vit)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:27:46.542269Z","iopub.execute_input":"2024-12-06T08:27:46.543074Z","iopub.status.idle":"2024-12-06T08:27:46.686501Z","shell.execute_reply.started":"2024-12-06T08:27:46.543037Z","shell.execute_reply":"2024-12-06T08:27:46.685290Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 300/300 [00:00<00:00, 4420.47it/s]\n100%|██████████| 300/300 [00:00<00:00, 4847.37it/s]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"#Infer of Densenet\ns_t = time.time()\npred_dense, index_dense = dense_model_infer(testload_dense)\n\np_dense = flatten(pred_dense)\np_dense_int = [i.item() for i in p_dense]\nidx_dense = flatten(index_dense)\nidx_dense_int = [i.item() for i in idx_dense]\ne_t = time.time()\nprint(f\"Infer time per image: {(e_t - s_t)/len(test_dense_li)*1000} ms\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:11.738410Z","iopub.execute_input":"2024-12-06T08:31:11.738875Z","iopub.status.idle":"2024-12-06T08:31:12.838425Z","shell.execute_reply.started":"2024-12-06T08:31:11.738833Z","shell.execute_reply":"2024-12-06T08:31:12.837270Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 10/10 [00:01<00:00,  9.24it/s]","output_type":"stream"},{"name":"stdout","text":"Infer time per image: 3.643615245819092 ms\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(f\"No. of fake images classified by Densenet: {int(np.array(p_dense_int).sum())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:27:50.502655Z","iopub.execute_input":"2024-12-06T08:27:50.503061Z","iopub.status.idle":"2024-12-06T08:27:50.509470Z","shell.execute_reply.started":"2024-12-06T08:27:50.503025Z","shell.execute_reply":"2024-12-06T08:27:50.508333Z"}},"outputs":[{"name":"stdout","text":"No. of fake images classified by Densenet: 138\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"#Infer of ViT\ns_t = time.time()\npred_vit, index_vit = vit_model_infer(testload_vit)\n\np_vit = flatten(pred_vit)\np_vit_int = [i.item() for i in p_vit]\nidx_vit = flatten(index_vit)\nidx_vit_int = [i.item() for i in idx_vit]\ne_t = time.time()\nprint(f\"Infer time per image: {(e_t - s_t)/len(test_vit_li)*1000} ms\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:21.488273Z","iopub.execute_input":"2024-12-06T08:31:21.488748Z","iopub.status.idle":"2024-12-06T08:31:30.892488Z","shell.execute_reply.started":"2024-12-06T08:31:21.488703Z","shell.execute_reply":"2024-12-06T08:31:30.891324Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 10/10 [00:09<00:00,  1.07it/s]","output_type":"stream"},{"name":"stdout","text":"Infer time per image: 31.324137846628826 ms\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(f\"No. of fake images classified by ViT: {int(np.array(p_vit_int).sum())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:34.148514Z","iopub.execute_input":"2024-12-06T08:31:34.149080Z","iopub.status.idle":"2024-12-06T08:31:34.157908Z","shell.execute_reply.started":"2024-12-06T08:31:34.149026Z","shell.execute_reply":"2024-12-06T08:31:34.156716Z"}},"outputs":[{"name":"stdout","text":"No. of fake images classified by ViT: 133\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"idx_dense_int == idx_vit_int","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:38.718193Z","iopub.execute_input":"2024-12-06T08:31:38.718609Z","iopub.status.idle":"2024-12-06T08:31:38.725596Z","shell.execute_reply.started":"2024-12-06T08:31:38.718557Z","shell.execute_reply":"2024-12-06T08:31:38.724347Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"# ***3. PATCHCRAFT***","metadata":{}},{"cell_type":"code","source":"# Set seed for NumPy\nnp.random.seed(42)\n\n# Set seed for Python's random module\nrandom.seed(42)\n\n# Set seed for PyTorch\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\ntorch.cuda.manual_seed_all(42)  # For multi-GPU setups\n\n# Ensure deterministic behavior in PyTorch\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Set environment variable for deterministic behavior\nos.environ['PYTHONHASHSEED'] = str(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:43.058037Z","iopub.execute_input":"2024-12-06T08:31:43.058425Z","iopub.status.idle":"2024-12-06T08:31:43.069686Z","shell.execute_reply.started":"2024-12-06T08:31:43.058395Z","shell.execute_reply":"2024-12-06T08:31:43.068561Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Define the original filter\nfilters = [\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, -1, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, -1, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 1, -1, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, -1, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, -1, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, -1, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, -1, 1, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, -1, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, -1, 0, 0],\n    [0, 0, 3, 0, 0],\n    [0, 0, -3, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [-1, 0, 0, 0, 0],\n    [0, 3, 0, 0, 0],\n    [0, 0, -3, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [-1, 3, -3, 1, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, -3, 0, 0],\n    [0, 3, 0, 0, 0],\n    [-1, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, -3, 0, 0],\n    [0, 0, 3, 0, 0],\n    [0, 0, -1, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, -3, 0, 0],\n    [0, 0, 0, 3, 0],\n    [0, 0, 0, 0, -1]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 1, -3, 3, -1],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, -1],\n    [0, 0, 0, 3, 0],\n    [0, 0, -3, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, -2, 0, 0],\n    [0, 0, 1, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, -2, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 1, -2, 1, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 1, 0],\n    [0, 0, -2, 0, 0],\n    [0, 1, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, -1, 2, -1, 0],\n    [0, 2, -4, 2, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, -1, 2, 0, 0],\n    [0, 2, -4, 0, 0],\n    [0, -1, 2, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0 ,0, 0, 0],\n    [0, 2, -4, 2, 0],\n    [0, -1, 2, -1, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 2, -1, 0],\n    [0, 0, -4, 2, 0],\n    [0, 0, 2, -1, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [-1, 2, -2, 2, -1],\n    [2, -6, 8, -6, 2],\n    [-2, 8, -12, 8, -2],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [-1, 2, -2, 0, 0],\n    [2, -6, 8, 0, 0],\n    [-2, 8, -12, 0, 0],\n    [2, -6, 8, 0, 0],\n    [-1, 2, -2, 0, 0]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [-2, 8, -12, 8, -2],\n    [2, -6, 8, -6, 2],\n    [-1, 2, -2, 2, -1]]),\n    np.array([\n    [0, 0, -2, 2, -1],\n    [0, 0, 8, -6, 2],\n    [0, 0, -12, 8, -2],\n    [0, 0, 8, -6, 2],\n    [0, 0, -2, 2, -1]]),\n    np.array([\n    [0, 0, 0, 0, 0],\n    [0, -1, 2, -1, 0],\n    [0, 2, -4, 2, 0],\n    [0, -1, 2, -1, 0],\n    [0, 0, 0, 0, 0]]),\n    np.array([\n    [-1, 2, -2, 2, -1],\n    [2, -6, 8, -6, 2],\n    [-2, 8, -12, 8, -2],\n    [2, -6, 8, -6, 2],\n    [-1, 2, -2, 2, -1]])\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:43.740025Z","iopub.execute_input":"2024-12-06T08:31:43.740455Z","iopub.status.idle":"2024-12-06T08:31:43.770563Z","shell.execute_reply.started":"2024-12-06T08:31:43.740419Z","shell.execute_reply":"2024-12-06T08:31:43.769263Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def normal_patches(image, patch_size):\n    height, width = image.shape[:2]\n    patches = [\n        image[i:i+patch_size, j:j+patch_size]\n        for i in range(0, height, patch_size)\n        for j in range(0, width, patch_size)\n    ]\n    return patches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:44.288090Z","iopub.execute_input":"2024-12-06T08:31:44.288489Z","iopub.status.idle":"2024-12-06T08:31:44.294399Z","shell.execute_reply.started":"2024-12-06T08:31:44.288453Z","shell.execute_reply":"2024-12-06T08:31:44.293301Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"def reconstruct(patches):\n    row_patches = np.concatenate([patch.reshape(-1, 3) for patch in patches], axis=0)\n    combined_image = row_patches.reshape(32, 16, 3)\n    return combined_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:45.809125Z","iopub.execute_input":"2024-12-06T08:31:45.809501Z","iopub.status.idle":"2024-12-06T08:31:45.815359Z","shell.execute_reply.started":"2024-12-06T08:31:45.809469Z","shell.execute_reply":"2024-12-06T08:31:45.813983Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def texture_diversity(image):\n    M = image.shape[0]\n    ldiv = 0\n\n    for channel in range(3):\n        patch = image[:, :, channel]\n        ldiv += np.abs(patch[:, :-1] - patch[:, 1:]).sum()\n        ldiv += np.abs(patch[:-1, :] - patch[1:, :]).sum()\n        ldiv += np.abs(patch[:-1, :-1] - patch[1:, 1:]).sum()\n        ldiv += np.abs(patch[1:, :-1] - patch[:-1, 1:]).sum()\n\n    return ldiv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:46.428098Z","iopub.execute_input":"2024-12-06T08:31:46.428492Z","iopub.status.idle":"2024-12-06T08:31:46.435953Z","shell.execute_reply.started":"2024-12-06T08:31:46.428460Z","shell.execute_reply":"2024-12-06T08:31:46.434776Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def plot_images(images):\n    num_images = len(images)  \n    plt.figure(figsize=(15, 5))\n    \n    for i, image in enumerate(images): \n        plt.subplot(1, num_images, i+1)\n        plt.imshow(image) \n        plt.axis('off')\n    \n    plt.show","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:46.957910Z","iopub.execute_input":"2024-12-06T08:31:46.958304Z","iopub.status.idle":"2024-12-06T08:31:46.963951Z","shell.execute_reply.started":"2024-12-06T08:31:46.958270Z","shell.execute_reply":"2024-12-06T08:31:46.962886Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def smash_reconstruction(image, patch_size=8, rich_ratio=0.5):\n    \n    patches = normal_patches(image, patch_size)\n    patches.sort(key=texture_diversity, reverse=True)\n    \n    # Split patches into rich and poor texture based on sorted texture diversity\n    num_rich = int(len(patches) * rich_ratio)\n    rich_texture = [patches[i] for i in range(num_rich)]\n    poor_texture = [patches[i] for i in range(num_rich, len(patches))]\n    \n    rich_image = reconstruct(rich_texture)\n    poor_image = reconstruct(poor_texture)\n    \n    return rich_image, poor_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:47.468064Z","iopub.execute_input":"2024-12-06T08:31:47.468441Z","iopub.status.idle":"2024-12-06T08:31:47.475489Z","shell.execute_reply.started":"2024-12-06T08:31:47.468411Z","shell.execute_reply":"2024-12-06T08:31:47.474319Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def high_pass(image):\n\n    filtered_images = []\n    for kernel in filters:\n        b_channel, g_channel, r_channel = cv2.split(image)\n        \n        b_filtered = cv2.filter2D(b_channel, -1, kernel)\n        g_filtered = cv2.filter2D(g_channel, -1, kernel)\n        r_filtered = cv2.filter2D(r_channel, -1, kernel)\n        \n        filtered_image = cv2.merge([b_filtered, g_filtered, r_filtered])\n        filtered_images.append(filtered_image)\n\n    return filtered_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:48.008138Z","iopub.execute_input":"2024-12-06T08:31:48.008558Z","iopub.status.idle":"2024-12-06T08:31:48.015548Z","shell.execute_reply.started":"2024-12-06T08:31:48.008520Z","shell.execute_reply":"2024-12-06T08:31:48.014317Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def concat_high_pass(filtered_images):\n    return np.concatenate(np.array(filtered_images), axis=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:48.619131Z","iopub.execute_input":"2024-12-06T08:31:48.619546Z","iopub.status.idle":"2024-12-06T08:31:48.624911Z","shell.execute_reply.started":"2024-12-06T08:31:48.619509Z","shell.execute_reply":"2024-12-06T08:31:48.623659Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def plot_images(image, filtered_images):\n    # Set up the figure for a row of subplots\n    num_images = len(filtered_images) + 1  # Original image + filtered images\n    plt.figure(figsize=(15, 5))\n    \n    # Plot the original image\n    plt.subplot(1, num_images, 1)\n    plt.title(\"Original\")\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) \n    plt.axis('off')\n    \n    # Plot each filtered image in a row\n    for i, filtered_image in enumerate(filtered_images, start=2):\n        plt.subplot(1, num_images, i)\n        plt.imshow(cv2.cvtColor(filtered_image, cv2.COLOR_BGR2RGB)) \n        plt.axis('off')\n\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:31:49.528941Z","iopub.execute_input":"2024-12-06T08:31:49.529343Z","iopub.status.idle":"2024-12-06T08:31:49.536615Z","shell.execute_reply.started":"2024-12-06T08:31:49.529310Z","shell.execute_reply":"2024-12-06T08:31:49.535385Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"test_path = \"/kaggle/working/perturbed_images_32_jpg\"\ntest_li = [(os.path.join(test_path, filename), int(filename.split(\".\")[0])) for filename in os.listdir(test_path) if filename.endswith(('.jpg', '.png', '.jpeg'))]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:32:25.378872Z","iopub.execute_input":"2024-12-06T08:32:25.379306Z","iopub.status.idle":"2024-12-06T08:32:25.386599Z","shell.execute_reply.started":"2024-12-06T08:32:25.379269Z","shell.execute_reply":"2024-12-06T08:32:25.385497Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"class eval_dataset(Dataset):\n    def __init__(self, li):\n        super().__init__()\n        self.li = li\n    \n    def __len__(self):\n        return len(self.li)\n    \n    def __getitem__(self, index):\n        image_path, idx = self.li[index]\n        img = cv2.imread(image_path)\n        rich_img, poor_img = smash_reconstruction(img, patch_size=16)\n        rich = torch.from_numpy(concat_high_pass(high_pass(rich_img)))\n        poor = torch.from_numpy(concat_high_pass(high_pass(poor_img)))\n        \n        return{\n            'rich' : rich.permute(2, 0, 1),\n            'poor' : poor.permute(2, 0, 1),\n            'index': idx\n        }\ninter_iit_test = eval_dataset(test_li)\neval_load = DataLoader(inter_iit_test,shuffle=False, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:32:28.370481Z","iopub.execute_input":"2024-12-06T08:32:28.370973Z","iopub.status.idle":"2024-12-06T08:32:28.379095Z","shell.execute_reply.started":"2024-12-06T08:32:28.370933Z","shell.execute_reply":"2024-12-06T08:32:28.377923Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"batch = next(iter(eval_load))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:32:29.309417Z","iopub.execute_input":"2024-12-06T08:32:29.310233Z","iopub.status.idle":"2024-12-06T08:32:29.400318Z","shell.execute_reply.started":"2024-12-06T08:32:29.310192Z","shell.execute_reply":"2024-12-06T08:32:29.399077Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"class Residual(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.poor_cnn = nn.Sequential(\n            nn.Conv2d(90, 64, 3, 1, 1),\n            nn.BatchNorm2d(64),\n            nn.Hardtanh()\n        )\n        self.rich_cnn = nn.Sequential(\n            nn.Conv2d(90, 64, 3, 1, 1),\n            nn.BatchNorm2d(64),\n            nn.Hardtanh()\n        )\n    \n    def forward(self, rich, poor):\n        rich = rich.float()\n        poor = poor.float()\n        rich = self.rich_cnn(rich)\n        poor = self.poor_cnn(poor)\n        residual = rich - poor\n        return residual\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:32:30.108672Z","iopub.execute_input":"2024-12-06T08:32:30.109072Z","iopub.status.idle":"2024-12-06T08:32:30.116644Z","shell.execute_reply.started":"2024-12-06T08:32:30.109030Z","shell.execute_reply":"2024-12-06T08:32:30.115453Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Sequential layers for the model\n        self.features = nn.Sequential(\n            # Convolutional Block 1\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),  # Input channels = 64, Output channels = 32\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            \n            # Average Pooling\n            nn.AvgPool2d(kernel_size=2),\n\n            # Convolutional Block 2\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            \n            # Average Pooling\n            nn.AvgPool2d(kernel_size=2),\n\n            # Convolutional Block 3\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            \n            # Average Pooling\n            nn.AvgPool2d(kernel_size=2),\n\n            # Convolutional Block 4\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            \n            nn.MaxPool2d(kernel_size=2)\n        )\n        \n        # Fully Connected Layer\n        self.fc = nn.Linear(64, 2)\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:32:30.818507Z","iopub.execute_input":"2024-12-06T08:32:30.818947Z","iopub.status.idle":"2024-12-06T08:32:30.829910Z","shell.execute_reply.started":"2024-12-06T08:32:30.818914Z","shell.execute_reply":"2024-12-06T08:32:30.828984Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"residual = Residual().to(device)\nclassifier = Classifier().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:32:31.529646Z","iopub.execute_input":"2024-12-06T08:32:31.530744Z","iopub.status.idle":"2024-12-06T08:32:31.543239Z","shell.execute_reply.started":"2024-12-06T08:32:31.530701Z","shell.execute_reply":"2024-12-06T08:32:31.542259Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"dict = torch.load('/kaggle/input/patchcraft/patchcraft_sd.pth', map_location = torch.device('cpu'))\ndict.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:32:33.009715Z","iopub.execute_input":"2024-12-06T08:32:33.010158Z","iopub.status.idle":"2024-12-06T08:32:33.042656Z","shell.execute_reply.started":"2024-12-06T08:32:33.010121Z","shell.execute_reply":"2024-12-06T08:32:33.041598Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"dict_keys(['residual_state_dict', 'classifier_state_dict'])"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"residual.load_state_dict(dict['residual_state_dict'])\nclassifier.load_state_dict(dict['classifier_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:32:34.528493Z","iopub.execute_input":"2024-12-06T08:32:34.528917Z","iopub.status.idle":"2024-12-06T08:32:34.539243Z","shell.execute_reply.started":"2024-12-06T08:32:34.528884Z","shell.execute_reply":"2024-12-06T08:32:34.538228Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"def eval(test_loader):\n    residual.eval()\n    classifier.eval()\n\n    pred = []\n    label = []\n    with torch.no_grad():\n        for batch in tqdm(test_loader):\n            label.append(batch['index'])\n            rich = batch['rich'].to(device)\n            poor = batch['poor'].to(device)\n\n            residual_output = residual(rich, poor)\n            predictions = classifier(residual_output).detach().cpu()\n            pred.append(predictions)\n\n    pred = torch.argmax(torch.nn.functional.softmax(torch.cat(pred,axis =0),dim = 1),dim=1)\n    idx = [j for i in label for j in i]\n    return pred, idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:32:35.538756Z","iopub.execute_input":"2024-12-06T08:32:35.539174Z","iopub.status.idle":"2024-12-06T08:32:35.547158Z","shell.execute_reply.started":"2024-12-06T08:32:35.539139Z","shell.execute_reply":"2024-12-06T08:32:35.545822Z"}},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":"# **Infer of patchcraft**","metadata":{}},{"cell_type":"code","source":"s_t = time.time()\npreds, idx_pc = eval(eval_load)\ne_t = time.time()\nprint(f\"Infer time per image: {((e_t - s_t)/300)*1000} ms\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:33:34.358523Z","iopub.execute_input":"2024-12-06T08:33:34.358913Z","iopub.status.idle":"2024-12-06T08:33:35.631909Z","shell.execute_reply.started":"2024-12-06T08:33:34.358880Z","shell.execute_reply":"2024-12-06T08:33:35.630792Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 10/10 [00:01<00:00,  7.94it/s]","output_type":"stream"},{"name":"stdout","text":"Infer time per image: 4.227723280588786 ms\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"p_patchcraft = list(preds.numpy())\nidx_pc_int = [i.item() for i in idx_pc]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:33:48.148622Z","iopub.execute_input":"2024-12-06T08:33:48.149019Z","iopub.status.idle":"2024-12-06T08:33:48.155225Z","shell.execute_reply.started":"2024-12-06T08:33:48.148986Z","shell.execute_reply":"2024-12-06T08:33:48.154016Z"},"scrolled":true},"outputs":[],"execution_count":56},{"cell_type":"code","source":"print(f\"No. of fake images classified by patchcraft: {np.array(p_patchcraft).sum()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:33:53.917433Z","iopub.execute_input":"2024-12-06T08:33:53.918296Z","iopub.status.idle":"2024-12-06T08:33:53.923323Z","shell.execute_reply.started":"2024-12-06T08:33:53.918256Z","shell.execute_reply":"2024-12-06T08:33:53.922245Z"}},"outputs":[{"name":"stdout","text":"No. of fake images classified by patchcraft: 156\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"idx_pc_int == idx_dense_int","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:33:56.547307Z","iopub.execute_input":"2024-12-06T08:33:56.547709Z","iopub.status.idle":"2024-12-06T08:33:56.554623Z","shell.execute_reply.started":"2024-12-06T08:33:56.547674Z","shell.execute_reply":"2024-12-06T08:33:56.553325Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"final_pred = hard_ensemble([p_patchcraft, p_dense_int, p_vit_int])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:34:00.857523Z","iopub.execute_input":"2024-12-06T08:34:00.858900Z","iopub.status.idle":"2024-12-06T08:34:00.866402Z","shell.execute_reply.started":"2024-12-06T08:34:00.858848Z","shell.execute_reply":"2024-12-06T08:34:00.865196Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"print(f\"No. of fake images classified by the Ensemble: {int(np.array(final_pred).sum())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:34:47.497746Z","iopub.execute_input":"2024-12-06T08:34:47.498197Z","iopub.status.idle":"2024-12-06T08:34:47.504572Z","shell.execute_reply.started":"2024-12-06T08:34:47.498158Z","shell.execute_reply":"2024-12-06T08:34:47.503353Z"}},"outputs":[{"name":"stdout","text":"No. of fake images classified by the Ensemble: 138\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"or_final_pred = or_gate([p_patchcraft, p_dense_int, p_vit_int])\nprint(f\"No. of fake images classified by the or gate ensemble: {int(np.array(or_final_pred).sum())}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:35:03.317534Z","iopub.execute_input":"2024-12-06T08:35:03.318355Z","iopub.status.idle":"2024-12-06T08:35:03.324433Z","shell.execute_reply.started":"2024-12-06T08:35:03.318314Z","shell.execute_reply":"2024-12-06T08:35:03.323352Z"}},"outputs":[{"name":"stdout","text":"No. of fake images classified by the or gate ensemble: 212\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"pred_dict = return_dict(or_final_pred, idx_dense_int)\nfinal_pred_dict = sorted(pred_dict, key=lambda x: x['index'])\n\nprint(final_pred_dict)","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-06T08:35:21.158895Z","iopub.execute_input":"2024-12-06T08:35:21.159884Z","iopub.status.idle":"2024-12-06T08:35:21.165961Z","shell.execute_reply.started":"2024-12-06T08:35:21.159831Z","shell.execute_reply":"2024-12-06T08:35:21.164695Z"}},"outputs":[{"name":"stdout","text":"[{'index': 1, 'prediction': 'fake'}, {'index': 2, 'prediction': 'fake'}, {'index': 3, 'prediction': 'real'}, {'index': 4, 'prediction': 'real'}, {'index': 5, 'prediction': 'fake'}, {'index': 6, 'prediction': 'real'}, {'index': 7, 'prediction': 'fake'}, {'index': 8, 'prediction': 'fake'}, {'index': 9, 'prediction': 'real'}, {'index': 10, 'prediction': 'real'}, {'index': 11, 'prediction': 'fake'}, {'index': 12, 'prediction': 'fake'}, {'index': 13, 'prediction': 'fake'}, {'index': 14, 'prediction': 'fake'}, {'index': 15, 'prediction': 'real'}, {'index': 16, 'prediction': 'real'}, {'index': 17, 'prediction': 'real'}, {'index': 18, 'prediction': 'fake'}, {'index': 19, 'prediction': 'fake'}, {'index': 20, 'prediction': 'fake'}, {'index': 21, 'prediction': 'fake'}, {'index': 22, 'prediction': 'fake'}, {'index': 23, 'prediction': 'fake'}, {'index': 24, 'prediction': 'fake'}, {'index': 25, 'prediction': 'fake'}, {'index': 26, 'prediction': 'fake'}, {'index': 27, 'prediction': 'fake'}, {'index': 28, 'prediction': 'fake'}, {'index': 29, 'prediction': 'fake'}, {'index': 30, 'prediction': 'fake'}, {'index': 31, 'prediction': 'fake'}, {'index': 32, 'prediction': 'fake'}, {'index': 33, 'prediction': 'fake'}, {'index': 34, 'prediction': 'fake'}, {'index': 35, 'prediction': 'fake'}, {'index': 36, 'prediction': 'fake'}, {'index': 37, 'prediction': 'fake'}, {'index': 38, 'prediction': 'fake'}, {'index': 39, 'prediction': 'fake'}, {'index': 40, 'prediction': 'fake'}, {'index': 41, 'prediction': 'real'}, {'index': 42, 'prediction': 'real'}, {'index': 43, 'prediction': 'fake'}, {'index': 44, 'prediction': 'fake'}, {'index': 45, 'prediction': 'real'}, {'index': 46, 'prediction': 'fake'}, {'index': 47, 'prediction': 'fake'}, {'index': 48, 'prediction': 'fake'}, {'index': 49, 'prediction': 'fake'}, {'index': 50, 'prediction': 'fake'}, {'index': 51, 'prediction': 'fake'}, {'index': 52, 'prediction': 'fake'}, {'index': 53, 'prediction': 'real'}, {'index': 54, 'prediction': 'fake'}, {'index': 55, 'prediction': 'fake'}, {'index': 56, 'prediction': 'fake'}, {'index': 57, 'prediction': 'fake'}, {'index': 58, 'prediction': 'fake'}, {'index': 59, 'prediction': 'real'}, {'index': 60, 'prediction': 'real'}, {'index': 61, 'prediction': 'fake'}, {'index': 62, 'prediction': 'fake'}, {'index': 63, 'prediction': 'fake'}, {'index': 64, 'prediction': 'fake'}, {'index': 65, 'prediction': 'fake'}, {'index': 66, 'prediction': 'fake'}, {'index': 67, 'prediction': 'fake'}, {'index': 68, 'prediction': 'fake'}, {'index': 69, 'prediction': 'fake'}, {'index': 70, 'prediction': 'fake'}, {'index': 71, 'prediction': 'fake'}, {'index': 72, 'prediction': 'fake'}, {'index': 73, 'prediction': 'real'}, {'index': 74, 'prediction': 'fake'}, {'index': 75, 'prediction': 'fake'}, {'index': 76, 'prediction': 'fake'}, {'index': 77, 'prediction': 'real'}, {'index': 78, 'prediction': 'fake'}, {'index': 79, 'prediction': 'fake'}, {'index': 80, 'prediction': 'fake'}, {'index': 81, 'prediction': 'real'}, {'index': 82, 'prediction': 'real'}, {'index': 83, 'prediction': 'real'}, {'index': 84, 'prediction': 'fake'}, {'index': 85, 'prediction': 'fake'}, {'index': 86, 'prediction': 'fake'}, {'index': 87, 'prediction': 'fake'}, {'index': 88, 'prediction': 'fake'}, {'index': 89, 'prediction': 'fake'}, {'index': 90, 'prediction': 'fake'}, {'index': 91, 'prediction': 'fake'}, {'index': 92, 'prediction': 'real'}, {'index': 93, 'prediction': 'fake'}, {'index': 94, 'prediction': 'fake'}, {'index': 95, 'prediction': 'real'}, {'index': 96, 'prediction': 'real'}, {'index': 97, 'prediction': 'fake'}, {'index': 98, 'prediction': 'fake'}, {'index': 99, 'prediction': 'fake'}, {'index': 100, 'prediction': 'real'}, {'index': 101, 'prediction': 'fake'}, {'index': 102, 'prediction': 'fake'}, {'index': 103, 'prediction': 'real'}, {'index': 104, 'prediction': 'fake'}, {'index': 105, 'prediction': 'fake'}, {'index': 106, 'prediction': 'real'}, {'index': 107, 'prediction': 'fake'}, {'index': 108, 'prediction': 'fake'}, {'index': 109, 'prediction': 'real'}, {'index': 110, 'prediction': 'fake'}, {'index': 111, 'prediction': 'fake'}, {'index': 112, 'prediction': 'real'}, {'index': 113, 'prediction': 'fake'}, {'index': 114, 'prediction': 'real'}, {'index': 115, 'prediction': 'fake'}, {'index': 116, 'prediction': 'fake'}, {'index': 117, 'prediction': 'fake'}, {'index': 118, 'prediction': 'fake'}, {'index': 119, 'prediction': 'fake'}, {'index': 120, 'prediction': 'real'}, {'index': 121, 'prediction': 'real'}, {'index': 122, 'prediction': 'fake'}, {'index': 123, 'prediction': 'fake'}, {'index': 124, 'prediction': 'real'}, {'index': 125, 'prediction': 'real'}, {'index': 126, 'prediction': 'real'}, {'index': 127, 'prediction': 'real'}, {'index': 128, 'prediction': 'real'}, {'index': 129, 'prediction': 'fake'}, {'index': 130, 'prediction': 'fake'}, {'index': 131, 'prediction': 'real'}, {'index': 132, 'prediction': 'fake'}, {'index': 133, 'prediction': 'fake'}, {'index': 134, 'prediction': 'fake'}, {'index': 135, 'prediction': 'real'}, {'index': 136, 'prediction': 'real'}, {'index': 137, 'prediction': 'fake'}, {'index': 138, 'prediction': 'real'}, {'index': 139, 'prediction': 'real'}, {'index': 140, 'prediction': 'fake'}, {'index': 141, 'prediction': 'real'}, {'index': 142, 'prediction': 'fake'}, {'index': 143, 'prediction': 'fake'}, {'index': 144, 'prediction': 'fake'}, {'index': 145, 'prediction': 'real'}, {'index': 146, 'prediction': 'real'}, {'index': 147, 'prediction': 'real'}, {'index': 148, 'prediction': 'fake'}, {'index': 149, 'prediction': 'real'}, {'index': 150, 'prediction': 'fake'}, {'index': 151, 'prediction': 'fake'}, {'index': 152, 'prediction': 'fake'}, {'index': 153, 'prediction': 'fake'}, {'index': 154, 'prediction': 'fake'}, {'index': 155, 'prediction': 'real'}, {'index': 156, 'prediction': 'fake'}, {'index': 157, 'prediction': 'fake'}, {'index': 158, 'prediction': 'fake'}, {'index': 159, 'prediction': 'fake'}, {'index': 160, 'prediction': 'fake'}, {'index': 161, 'prediction': 'fake'}, {'index': 162, 'prediction': 'fake'}, {'index': 163, 'prediction': 'fake'}, {'index': 164, 'prediction': 'fake'}, {'index': 165, 'prediction': 'fake'}, {'index': 166, 'prediction': 'fake'}, {'index': 167, 'prediction': 'fake'}, {'index': 168, 'prediction': 'fake'}, {'index': 169, 'prediction': 'fake'}, {'index': 170, 'prediction': 'fake'}, {'index': 171, 'prediction': 'fake'}, {'index': 172, 'prediction': 'real'}, {'index': 173, 'prediction': 'real'}, {'index': 174, 'prediction': 'fake'}, {'index': 175, 'prediction': 'fake'}, {'index': 176, 'prediction': 'real'}, {'index': 177, 'prediction': 'fake'}, {'index': 178, 'prediction': 'fake'}, {'index': 179, 'prediction': 'real'}, {'index': 180, 'prediction': 'real'}, {'index': 181, 'prediction': 'fake'}, {'index': 182, 'prediction': 'fake'}, {'index': 183, 'prediction': 'fake'}, {'index': 184, 'prediction': 'fake'}, {'index': 185, 'prediction': 'fake'}, {'index': 186, 'prediction': 'fake'}, {'index': 187, 'prediction': 'real'}, {'index': 188, 'prediction': 'real'}, {'index': 189, 'prediction': 'fake'}, {'index': 190, 'prediction': 'fake'}, {'index': 191, 'prediction': 'fake'}, {'index': 192, 'prediction': 'fake'}, {'index': 193, 'prediction': 'fake'}, {'index': 194, 'prediction': 'fake'}, {'index': 195, 'prediction': 'fake'}, {'index': 196, 'prediction': 'fake'}, {'index': 197, 'prediction': 'fake'}, {'index': 198, 'prediction': 'fake'}, {'index': 199, 'prediction': 'fake'}, {'index': 200, 'prediction': 'fake'}, {'index': 201, 'prediction': 'real'}, {'index': 202, 'prediction': 'real'}, {'index': 203, 'prediction': 'fake'}, {'index': 204, 'prediction': 'fake'}, {'index': 205, 'prediction': 'fake'}, {'index': 206, 'prediction': 'fake'}, {'index': 207, 'prediction': 'fake'}, {'index': 208, 'prediction': 'fake'}, {'index': 209, 'prediction': 'fake'}, {'index': 210, 'prediction': 'real'}, {'index': 211, 'prediction': 'fake'}, {'index': 212, 'prediction': 'fake'}, {'index': 213, 'prediction': 'fake'}, {'index': 214, 'prediction': 'real'}, {'index': 215, 'prediction': 'real'}, {'index': 216, 'prediction': 'fake'}, {'index': 217, 'prediction': 'fake'}, {'index': 218, 'prediction': 'fake'}, {'index': 219, 'prediction': 'fake'}, {'index': 220, 'prediction': 'fake'}, {'index': 221, 'prediction': 'real'}, {'index': 222, 'prediction': 'fake'}, {'index': 223, 'prediction': 'fake'}, {'index': 224, 'prediction': 'fake'}, {'index': 225, 'prediction': 'fake'}, {'index': 226, 'prediction': 'real'}, {'index': 227, 'prediction': 'fake'}, {'index': 228, 'prediction': 'fake'}, {'index': 229, 'prediction': 'fake'}, {'index': 230, 'prediction': 'real'}, {'index': 231, 'prediction': 'fake'}, {'index': 232, 'prediction': 'fake'}, {'index': 233, 'prediction': 'fake'}, {'index': 234, 'prediction': 'fake'}, {'index': 235, 'prediction': 'fake'}, {'index': 236, 'prediction': 'real'}, {'index': 237, 'prediction': 'fake'}, {'index': 238, 'prediction': 'fake'}, {'index': 239, 'prediction': 'fake'}, {'index': 240, 'prediction': 'fake'}, {'index': 241, 'prediction': 'real'}, {'index': 242, 'prediction': 'real'}, {'index': 243, 'prediction': 'real'}, {'index': 244, 'prediction': 'real'}, {'index': 245, 'prediction': 'fake'}, {'index': 246, 'prediction': 'fake'}, {'index': 247, 'prediction': 'real'}, {'index': 248, 'prediction': 'real'}, {'index': 249, 'prediction': 'fake'}, {'index': 250, 'prediction': 'fake'}, {'index': 251, 'prediction': 'fake'}, {'index': 252, 'prediction': 'real'}, {'index': 253, 'prediction': 'fake'}, {'index': 254, 'prediction': 'real'}, {'index': 255, 'prediction': 'fake'}, {'index': 256, 'prediction': 'fake'}, {'index': 257, 'prediction': 'fake'}, {'index': 258, 'prediction': 'real'}, {'index': 259, 'prediction': 'real'}, {'index': 260, 'prediction': 'real'}, {'index': 261, 'prediction': 'real'}, {'index': 262, 'prediction': 'real'}, {'index': 263, 'prediction': 'fake'}, {'index': 264, 'prediction': 'fake'}, {'index': 265, 'prediction': 'real'}, {'index': 266, 'prediction': 'fake'}, {'index': 267, 'prediction': 'fake'}, {'index': 268, 'prediction': 'fake'}, {'index': 269, 'prediction': 'fake'}, {'index': 270, 'prediction': 'real'}, {'index': 271, 'prediction': 'real'}, {'index': 272, 'prediction': 'fake'}, {'index': 273, 'prediction': 'fake'}, {'index': 274, 'prediction': 'real'}, {'index': 275, 'prediction': 'fake'}, {'index': 276, 'prediction': 'fake'}, {'index': 277, 'prediction': 'fake'}, {'index': 278, 'prediction': 'fake'}, {'index': 279, 'prediction': 'real'}, {'index': 280, 'prediction': 'fake'}, {'index': 281, 'prediction': 'fake'}, {'index': 282, 'prediction': 'real'}, {'index': 283, 'prediction': 'fake'}, {'index': 284, 'prediction': 'fake'}, {'index': 285, 'prediction': 'fake'}, {'index': 286, 'prediction': 'fake'}, {'index': 287, 'prediction': 'fake'}, {'index': 288, 'prediction': 'real'}, {'index': 289, 'prediction': 'real'}, {'index': 290, 'prediction': 'real'}, {'index': 291, 'prediction': 'real'}, {'index': 292, 'prediction': 'fake'}, {'index': 293, 'prediction': 'fake'}, {'index': 294, 'prediction': 'fake'}, {'index': 295, 'prediction': 'fake'}, {'index': 296, 'prediction': 'real'}, {'index': 297, 'prediction': 'real'}, {'index': 298, 'prediction': 'real'}, {'index': 299, 'prediction': 'fake'}, {'index': 300, 'prediction': 'fake'}]\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# Path to save the JSON file\noutput_path = '/kaggle/working/53_task1.json'\n\n# Save the list of dictionaries as a JSON file\nwith open(output_path, 'w') as json_file:\n    json.dump(final_pred_dict, json_file, indent=1)\n\nprint(f\"File saved to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:35:47.699093Z","iopub.execute_input":"2024-12-06T08:35:47.699535Z","iopub.status.idle":"2024-12-06T08:35:47.710685Z","shell.execute_reply.started":"2024-12-06T08:35:47.699497Z","shell.execute_reply":"2024-12-06T08:35:47.709334Z"}},"outputs":[{"name":"stdout","text":"File saved to: /kaggle/working/53_task1.json\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}