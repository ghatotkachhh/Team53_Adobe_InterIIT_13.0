{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726},{"sourceId":10105691,"sourceType":"datasetVersion","datasetId":6233623}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install qwen-vl-utils\n!pip install jsonformer","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport time\nimport torch\nfrom PIL import Image\nimport requests\nimport matplotlib.pyplot as plt\nfrom transformers import Qwen2VLForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer, AutoProcessor, AutoModelForVision2Seq, BitsAndBytesConfig\nfrom qwen_vl_utils import process_vision_info\nimport os\nimport gc\nfrom tqdm.notebook import tqdm_notebook as tqdm   \nimport json\nimport re\nfrom jsonformer import Jsonformer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_id =\"/kaggle/input/qwen7bfinetuned/qwen2-7b-instruct-artifact\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n \nmodel = Qwen2VLForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\",quantization_config=bnb_config)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"artifacts_text = \"\"\"\n- Inconsistent object boundaries\n- Discontinuous surfaces\n- Non-manifold geometries in rigid structures\n- Floating or disconnected components\n- Asymmetric features in naturally symmetric objects \n- Misaligned bilateral elements in animal faces \n- Irregular proportions in mechanical components \n- Texture bleeding between adjacent regions\n- Texture repetition patterns\n- Over-smoothing of natural textures \n- Artificial noise patterns in uniform surfaces\n- Unrealistic specular highlights\n- Inconsistent material properties\n- Metallic surface artifacts \n- Dental anomalies in mammals \n- Anatomically incorrect paw structures\n- Improper fur direction flows\n- Unrealistic eye reflections\n- Misshapen ears or appendages\n- Impossible mechanical connections\n- Inconsistent scale of mechanical parts\n- Physically impossible structural elements\n- Inconsistent shadow directions\n- Multiple light source conflicts\n- Missing ambient occlusion\n- Incorrect reflection mapping\n- Incorrect perspective rendering\n- Scale inconsistencies within single objects\n- Spatial relationship errors\n- Depth perception anomalies\n- Over-sharpening artifacts\n- Aliasing along high-contrast edges\n- Blurred boundaries in fine details\n- Jagged edges in curved structures\n- Random noise patterns in detailed areas\n- Loss of fine detail in complex structures\n- Artificial enhancement artifacts\n- Incorrect wheel geometry\n- Implausible aerodynamic structures\n- Misaligned body panels\n- Impossible mechanical joints\n- Distorted window reflections\n- Anatomically impossible joint configurations\n- Unnatural pose artifacts\n- Biological asymmetry errors\n- Regular grid-like artifacts in textures\n- Repeated element patterns\n- Systematic color distribution anomalies\n- Frequency domain signatures\n- Color coherence breaks\n- Unnatural color transitions\n- Resolution inconsistencies within regions\n- Unnatural Lighting Gradients\n- Incorrect Skin Tones\n- Fake depth of field\n- Abruptly cut off objects\n- Glow or light bleed around object boundaries\n- Ghosting effects: Semi-transparent duplicates of elements\n- Cinematization Effects\n- Excessive sharpness in certain image regions\n- Artificial smoothness\n- Movie-poster like composition of ordinary scenes\n- Dramatic lighting that defies natural physics\n- Artificial depth of field in object presentation\n- Unnaturally glossy surfaces\n- Synthetic material appearance\n- Multiple inconsistent shadow sources\n- Exaggerated characteristic features\n- Impossible foreshortening in animal bodies\n- Scale inconsistencies within the same object class\n\"\"\"\n\nartifacts_list = [line.strip(\"- \").strip() for line in artifacts_text.strip().split(\"\\n\") if line.strip()]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_image_and_prompt(model, processor, image_path, prompt, max_tokens=1024):\n    # Load and display the image\n    if image_path.startswith('http'):\n        image = Image.open(requests.get(image_path, stream=True).raw)\n    else:\n        image = Image.open(image_path)\n    \n    # Prepare messages\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\" : [\n                {\"type\": \"text\", \"text\": \"You are an expert in identifying and analyzing artifacts that indicate why the image may appear unnatural or fake\"}\n            ]\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": image},\n                {\"type\": \"text\", \"text\": prompt}\n            ]\n        }\n    ]\n    \n    # Process input\n    input_text = processor.apply_chat_template(messages, tokenize = False, add_generation_prompt=True)\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(\n        text = input_text,\n        images = image_inputs,\n        videos = video_inputs,\n        padding = True,\n        add_special_tokens=False,\n        return_tensors=\"pt\"\n    ).to(model.device)\n    \n    # Generate output\n    with torch.inference_mode():\n        output = model.generate(**inputs, temperature = 0.1, max_new_tokens=max_tokens)\n    \n    # Decode output\n        generated_ids_trimmed = [\n            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, output)\n        ]\n        response = processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )\n    \n    # Clean up memory\n    del inputs, output\n    \n    return response","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_explanations(imagelink):\n    # Start timing\n    start = time.time()\n    \n    # Display the image\n    img = Image.open(imagelink)\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title('Input Image')\n    plt.show()\n\n    artifact_explanations = {}\n\n    for artifact in tqdm(artifacts_list, desc=\"Processing: \"):\n        a1 = time.time()\n        \n        prompt = f\"\"\"TASK: For the provided artifact, Generate 1-2 explanation for the artifact. \n            IF the artifact is not applicable to the image, print None and NOTHING ELSE. Explain strictly in context of the given image. \n            Limit your explanation to 64 tokens.\n            artifact : {artifact}\"\"\"\n        artifact_response = process_image_and_prompt(model, processor, imagelink, prompt, 64)[0]\n        # print(f\"------------------------ {artifact} ------------------------\")\n        # print(artifact_response)\n        # print()\n        artifact_explanations[artifact] = artifact_response\n        \n        a2 = time.time()    \n        # print(f\"\\nExplanation processing time: {a2 - a1} seconds\")\n        \n    # Print total processing time\n    end = time.time()\n    print(f\"\\nTotal processing time: {end - start} seconds\")\n\n    return artifact_explanations","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"artifact_explanations = get_explanations(\"/kaggle/input/testdataadobe/perturbed_images_32/67.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_dir = \"/kaggle/input/testdataadobe/perturbed_images_32\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_paths = []\n\nstart = 1\nend = 301\nfor i in range(start, end):\n    image_paths.append(image_dir + \"/\" + str(i) + \".png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_explanations = {}\nfor path in tqdm(image_paths):\n    artifact_explanations = get_explanations(path)\n    index = key.split('/')[-1].split('.')[0]\n    all_explanations[path] = group_responses\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}